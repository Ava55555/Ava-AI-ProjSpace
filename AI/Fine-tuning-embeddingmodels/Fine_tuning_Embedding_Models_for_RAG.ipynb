{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckbbj5diaHkg"
   },
   "source": [
    "# Fine-tuning Embedding Models\n",
    "\n",
    "  1. Dependencies and Boilerplate\n",
    "  2. Loading Data\n",
    "  3. Constructing a Fine-tuning Dataset\n",
    "  4. Fine-tuning `snowflake-arctic-embed-m`\n",
    "  5. Evaluating our Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NkSaurzbpyS"
   },
   "source": [
    "## Task 1: Dependencies and Boilerplate\n",
    "\n",
    "I will set up our `nest_asyncio` to leverage async loops "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c_EUibmcDU3"
   },
   "source": [
    "### Nest Asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zq-6s7LbPnKH"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8uFz8RVcFFu"
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulZIBA1ZoSsV",
    "outputId": "c6ec4eac-d9d6-4fa8-a1ad-20ff823632e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m372.0/372.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.7/987.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.6/328.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.0/135.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain_openai langchain_huggingface langchain_core langchain langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3GFD7B-tOCrx",
    "outputId": "8c36d3ff-349b-401f-b522-9c2d0420efc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU pymupdf faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FM-eUlrcI8a"
   },
   "source": [
    "### Provide OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wA_mlurVqtrp",
    "outputId": "f5cd4d89-346f-4a16-878f-5a4834fb331e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your OpenAI API Key: ··········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFZ217gCDVTr"
   },
   "source": [
    "## Task 2: Loading Data\n",
    "\n",
    "The data can be found in [this GitHub repo](https://github.com/AI-Maker-Space/DataRepository/tree/main/high-performance-rag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9z9tZVAedkk",
    "outputId": "ab9fb8fe-d2d6-4d48-b08b-7684ef601bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DataRepository'...\n",
      "remote: Enumerating objects: 78, done.\u001b[K\n",
      "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
      "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
      "remote: Total 78 (delta 21), reused 28 (delta 8), pack-reused 8\u001b[K\n",
      "Receiving objects: 100% (78/78), 69.86 MiB | 28.35 MiB/s, done.\n",
      "Resolving deltas: 100% (21/21), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AI-Maker-Space/DataRepository.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jr3uSmKQPAWG",
    "outputId": "f74df5ab-6cf6-4b66-e630-ef343bc47c12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/DataRepository\n"
     ]
    }
   ],
   "source": [
    "%cd DataRepository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DHJhTzsvN75t"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "training_documents_loaded = PyMuPDFLoader(\"MuskComplaint.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NsPrOOqXOsNX"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 250,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OMYPX6N6Os8M"
   },
   "outputs": [],
   "source": [
    "training_documents = text_splitter.split_documents(training_documents_loaded.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAozuMoNOvnp",
    "outputId": "a7907273-15e2-4a0b-8778-07351b2f1667"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AwyIForybIpo"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "id_set = set()\n",
    "\n",
    "for document in training_documents:\n",
    "  id = str(uuid.uuid4())\n",
    "  while id in id_set:\n",
    "    id = uuid.uuid4()\n",
    "  id_set.add(id)\n",
    "  document.metadata[\"id\"] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MTS4GTSEcnG4"
   },
   "outputs": [],
   "source": [
    "training_split_documents = training_documents[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Rxv2OlO6Oxjz"
   },
   "outputs": [],
   "source": [
    "val_split_documents = training_documents[300:350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "I0RmbqEWS-SR"
   },
   "outputs": [],
   "source": [
    "test_split_documents = training_documents[350:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzlvKbONDWvQ"
   },
   "source": [
    "## Task 3: Constructing a Fine-tuning Dataset\n",
    "\n",
    "Using the nodes we created above, we can finally start constructing a fine-tuning dataset utilizing OpenAI's `gpt-4o-mini` \n",
    "The basic idea is\n",
    "\n",
    "1. look at a document\n",
    "2. generate questions that could be answered by that node\n",
    "\n",
    "This gives us a number of question/context pairs that we can use to fine-tune our Embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_EWfmIscMrvg"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "qa_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-hLnsSB6Y-S"
   },
   "source": [
    "create a simple Question Generation prompt to query `gpt-4o-mini` to generate Questions for each retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "diEWcw00NMSj"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_prompt = \"\"\"\\\n",
    "Given the following context, you must generate questions based on only the provided context.\n",
    "\n",
    "You are to generate {n_questions} questions which should be provided in the following format:\n",
    "\n",
    "1. QUESTION #1\n",
    "2. QUESTION #2\n",
    "...\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u87Izpgm6_fk"
   },
   "source": [
    "create a simple chain to query the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ggl9SSjiNbpG"
   },
   "outputs": [],
   "source": [
    "question_generation_chain = qa_prompt_template | qa_chat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4duvHirh7DQv"
   },
   "source": [
    "There's a lot going on in this function - let's take a deeper look:\n",
    "\n",
    "1. First, we provide a list of documents and a number of questions\n",
    "2. We, for each document in our list, generate `n_questions` of questions.\n",
    "3. We then associate those questions and contexts via a `UUID`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "KbzkPTPVPH-I"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def create_questions(documents, n_questions):\n",
    "  questions = {}\n",
    "  relevant_docs = {}\n",
    "  for document in tqdm.tqdm(documents):\n",
    "    document_content = {\"context\" : document.page_content, \"questions\" : []}\n",
    "    questions_generated = question_generation_chain.invoke({\"context\": document.page_content, \"n_questions\": n_questions})\n",
    "    for question in questions_generated.content.split(\"\\n\"):\n",
    "      question_id = str(uuid.uuid4())\n",
    "      questions[question_id] = \"\".join(question.split(\".\")[1:]).strip()\n",
    "      relevant_docs[question_id] = [document.metadata[\"id\"]]\n",
    "  return questions, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FSTG0bb7w73"
   },
   "source": [
    "We'll use the function to generate training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Ex3LzsfPQEM",
    "outputId": "004198da-5f7c-4c00-8ebf-edd494bfd5ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:28<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "training_questions, training_relevant_contexts = create_questions(training_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIZm4CqGVzBx",
    "outputId": "02b19626-17b7-4f9b-c2b6-2096432a8113"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:33<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "val_questions, val_relevant_contexts = create_questions(val_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6qUHg9sV2_y",
    "outputId": "54144261-0f88-4259-8531-899918babece"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:33<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "test_questions, test_relevant_contexts = create_questions(test_split_documents, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0U959IN71dH"
   },
   "source": [
    "We'll save each dataset for use late - if it's required.\n",
    "\n",
    "> NOTE: These datasets will be provided in the repository in case of any issues with the data generation steps or want to save API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "iF6IFFq9VsNu"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
    "\n",
    "train_dataset = {\n",
    "    \"questions\" : training_questions,\n",
    "    \"relevant_contexts\" : training_relevant_contexts,\n",
    "    \"corpus\" : training_corpus\n",
    "}\n",
    "\n",
    "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PqF9WaueV-V8"
   },
   "outputs": [],
   "source": [
    "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
    "\n",
    "val_dataset = {\n",
    "    \"questions\" : val_questions,\n",
    "    \"relevant_contexts\" : val_relevant_contexts,\n",
    "    \"corpus\" : val_corpus\n",
    "}\n",
    "\n",
    "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "0DSQ7WMnWAu6"
   },
   "outputs": [],
   "source": [
    "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
    "\n",
    "test_dataset = {\n",
    "    \"questions\" : test_questions,\n",
    "    \"relevant_contexts\" : test_relevant_contexts,\n",
    "    \"corpus\" : train_corpus\n",
    "}\n",
    "\n",
    "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAwklqQCgVi-"
   },
   "source": [
    "## Task 4: Fine-tuning `snowflake-arctic-embed-m`\n",
    "\n",
    "Now that we have a dataset, let's grab a `sentence-transformers` Embeddings model\n",
    "\n",
    "We'll be using Snowflake's as a base embeddings model.\n",
    "\n",
    "It is a well performing embeddings model by itself, but there's a lot of very specific domain terms and vocabulary in our courpus - so lets fine-tune it and see what that can do for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AXzVHP3v1Cno",
    "outputId": "02028873-e939-4562-ccc8-c24e141939c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
      "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU sentence_transformers datasets pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-PGsQB7Xo6V",
    "outputId": "41ddc960-c958-46f0-8a7a-6926983aa099"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_id = \"Snowflake/snowflake-arctic-embed-m\"\n",
    "model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ztG07iB8CFO"
   },
   "source": [
    "We'll grab some necessary imports from `sentence_transformers` and `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "B-WbpuUWYFJr"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sentence_transformers import InputExample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJtPPlck8HBE"
   },
   "source": [
    "We're using a toy batch size here to reflect the limited number of examples we have.\n",
    "\n",
    "Ideally we'd want to use a much larger batch size. (~64+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8Lokhy6KYHAv"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-6DT8hc8PmT"
   },
   "source": [
    "Let's move our dataset into the expected format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "JJk37zQsYJ4P"
   },
   "outputs": [],
   "source": [
    "corpus = train_dataset['corpus']\n",
    "queries = train_dataset['questions']\n",
    "relevant_docs = train_dataset['relevant_contexts']\n",
    "\n",
    "examples = []\n",
    "for query_id, query in queries.items():\n",
    "    doc_id = relevant_docs[query_id][0]\n",
    "    text = corpus[doc_id]\n",
    "    example = InputExample(texts=[query, text])\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjFx7KHI8TL0"
   },
   "source": [
    "Now we can create a `torch` `DataLoader`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tiizmeIqZ_-w"
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    examples, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vA8rzlX8XbT"
   },
   "source": [
    "prepare our loss function!\n",
    "\n",
    "The loss we're using today is called `MultipleNegativesRankingLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "OLEO55iSaBiN"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import losses\n",
    "loss = losses.MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_S688xAk8sip"
   },
   "source": [
    "#### ❓ Question #1:\n",
    "\n",
    "How does this loss work? Can you explain what samples are positive vs. what samples are negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKxRuXfH844c"
   },
   "source": [
    "Now we can set-up our evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "f0hAFwUyaHQG"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "corpus = val_dataset['corpus']\n",
    "queries = val_dataset['questions']\n",
    "relevant_docs = val_dataset['relevant_contexts']\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYfap_ct8-bU"
   },
   "source": [
    "We'll train this model for 5 epochs, though you could increase this number if we had a significant amount more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "svZG0pBHiQr6"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxitWoNX9DwW"
   },
   "source": [
    "It's training time!\n",
    "\n",
    "> NOTE: We're manually defining a warm-up period here - this is just to provide a smooth ramp into our training....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332,
     "referenced_widgets": [
      "b99ccc41a6ff4383bf1781e2fbb2591e",
      "3701922ddbc147af910500d8bb33f6d8",
      "ba5e99343a5e4c0db92a99f14b34a355",
      "7d12f352771742d5a9f4f6ba1fdb28f7",
      "eef701e52c0447c093b7461200acde71",
      "4cbc442b299f4394be984e857b1c9637",
      "05aed3d3076b47f6b668756f128b8e96",
      "66b4c731c11f4fd3badfb17f2c5fed4a",
      "a4b2f6b5744440b89886e2c34a1eca2d",
      "8ead19d34b6543fd9d054901101f0df9",
      "db63a9a43a034cf1b4c5113b011e1f2f"
     ]
    },
    "id": "aDhUHZY-iR09",
    "outputId": "e5403a2c-22ab-4495-e4b4-9b7dffdbe9dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cosine Accuracy@1</th>\n",
       "      <th>Cosine Accuracy@3</th>\n",
       "      <th>Cosine Accuracy@5</th>\n",
       "      <th>Cosine Accuracy@10</th>\n",
       "      <th>Cosine Precision@1</th>\n",
       "      <th>Cosine Precision@3</th>\n",
       "      <th>Cosine Precision@5</th>\n",
       "      <th>Cosine Precision@10</th>\n",
       "      <th>Cosine Recall@1</th>\n",
       "      <th>Cosine Recall@3</th>\n",
       "      <th>Cosine Recall@5</th>\n",
       "      <th>Cosine Recall@10</th>\n",
       "      <th>Cosine Ndcg@10</th>\n",
       "      <th>Cosine Mrr@10</th>\n",
       "      <th>Cosine Map@100</th>\n",
       "      <th>Dot Accuracy@1</th>\n",
       "      <th>Dot Accuracy@3</th>\n",
       "      <th>Dot Accuracy@5</th>\n",
       "      <th>Dot Accuracy@10</th>\n",
       "      <th>Dot Precision@1</th>\n",
       "      <th>Dot Precision@3</th>\n",
       "      <th>Dot Precision@5</th>\n",
       "      <th>Dot Precision@10</th>\n",
       "      <th>Dot Recall@1</th>\n",
       "      <th>Dot Recall@3</th>\n",
       "      <th>Dot Recall@5</th>\n",
       "      <th>Dot Recall@10</th>\n",
       "      <th>Dot Ndcg@10</th>\n",
       "      <th>Dot Mrr@10</th>\n",
       "      <th>Dot Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.937403</td>\n",
       "      <td>0.920667</td>\n",
       "      <td>0.921576</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.937403</td>\n",
       "      <td>0.920667</td>\n",
       "      <td>0.921576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.948969</td>\n",
       "      <td>0.935667</td>\n",
       "      <td>0.936576</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.948969</td>\n",
       "      <td>0.935667</td>\n",
       "      <td>0.936576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.950278</td>\n",
       "      <td>0.937333</td>\n",
       "      <td>0.938242</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.950278</td>\n",
       "      <td>0.937333</td>\n",
       "      <td>0.938242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.950972</td>\n",
       "      <td>0.938167</td>\n",
       "      <td>0.939076</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.950972</td>\n",
       "      <td>0.938167</td>\n",
       "      <td>0.939076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.950972</td>\n",
       "      <td>0.938167</td>\n",
       "      <td>0.939076</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.950972</td>\n",
       "      <td>0.938167</td>\n",
       "      <td>0.939076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.952281</td>\n",
       "      <td>0.939833</td>\n",
       "      <td>0.940742</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.952281</td>\n",
       "      <td>0.939833</td>\n",
       "      <td>0.940742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.950972</td>\n",
       "      <td>0.938167</td>\n",
       "      <td>0.939076</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.950972</td>\n",
       "      <td>0.938167</td>\n",
       "      <td>0.939076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99ccc41a6ff4383bf1781e2fbb2591e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(loader, loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='finetuned_arctic',\n",
    "    show_progress_bar=True,\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bo0zW5k9Poq"
   },
   "source": [
    "## Task 5: Evaluating our Retriever\n",
    "\n",
    "Now that we have fine-tuned our retriever - let's see if it's worthwhile the effort\n",
    "\n",
    "We'll start with some basic imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Vq-2oqU0wHFr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jD0qrIh9X8f"
   },
   "source": [
    "Now we'll define a function that will help us evaluate our retrieval process.\n",
    "\n",
    "> NOTE: We're assuming 1 correct document in a \"hit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "0713_3cowX4q"
   },
   "outputs": [],
   "source": [
    "def evaluate_openai(\n",
    "    dataset,\n",
    "    embed_model,\n",
    "    top_k=5,\n",
    "    verbose=False,\n",
    "):\n",
    "  corpus = dataset['corpus']\n",
    "  questions = dataset['questions']\n",
    "  relevant_docs = dataset['relevant_contexts']\n",
    "  documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n",
    "  vectorstore = FAISS.from_documents(documents, embed_model)\n",
    "\n",
    "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "\n",
    "  eval_results = []\n",
    "  for id, question in tqdm.tqdm(questions.items()):\n",
    "    retrieved_nodes = retriever.invoke(question)\n",
    "    retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n",
    "    expected_id = relevant_docs[id][0]\n",
    "    is_hit = expected_id in retrieved_ids\n",
    "    eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n",
    "\n",
    "  return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOr49m4O9lxY"
   },
   "source": [
    "All that's left to do is evaluate, we'll evaluate our model against:\n",
    "\n",
    "1. OpenAI's closed source `text-embedding-3-small`\n",
    "2. The base non-fine-tuned version of `Snowflake/snowflake-arctic-embed-m`.\n",
    "\n",
    "Let's see how it stacks up...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijaeYpf593IW"
   },
   "source": [
    "### `text-embedding-3-small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kyY3PztaxnU3",
    "outputId": "4c18f521-fc11-4ed5-c673-e22c3b02dece"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.19it/s]\n"
     ]
    }
   ],
   "source": [
    "te3_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "te3_results = evaluate_openai(test_dataset, te3_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "kkyW90TCxx_i"
   },
   "outputs": [],
   "source": [
    "te3_results_df = pd.DataFrame(te3_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MscVRdNCylJ-",
    "outputId": "6759f80a-da27-4623-eb3a-ea1b4ca64394"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
    "te3_hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ra-mh0L96dQ"
   },
   "source": [
    "### `Snowflake/snowflake-arctic-embed-m` (base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEskxwvFypHe",
    "outputId": "7eca3d0b-cc48-40b2-c78e-e6ac5437a93d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 81.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-m\")\n",
    "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "KlKgiXTWzMTg"
   },
   "outputs": [],
   "source": [
    "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zV5vJWrJzOhc",
    "outputId": "ad9559a3-bbd4-4e25-96d6-bb5caaab9689"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
    "arctic_embed_m_hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcR3-0s19_lu"
   },
   "source": [
    "### `Snowflake/snowflake-arctic-embed-m` (fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ilse1LduzP1i",
    "outputId": "c1091e41-7962-43d8-a6ad-b0b3ba6b251e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at finetuned_arctic and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 100/100 [00:01<00:00, 80.02it/s]\n"
     ]
    }
   ],
   "source": [
    "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic\")\n",
    "finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "xxhZPqkNzZlh"
   },
   "outputs": [],
   "source": [
    "finetune_results_df = pd.DataFrame(finetune_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4thAK2BXzaj6",
    "outputId": "95b1eb46-4b63-4068-bc67-67f1e741b610"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n",
    "finetune_hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-aKOB1m-Bi8"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "As you can see - with only a few hundred data points, we're able to increase our embedding model to outperform even OpenAI's closed source solution"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05aed3d3076b47f6b668756f128b8e96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3701922ddbc147af910500d8bb33f6d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cbc442b299f4394be984e857b1c9637",
      "placeholder": "​",
      "style": "IPY_MODEL_05aed3d3076b47f6b668756f128b8e96",
      "value": "Computing widget examples:   0%"
     }
    },
    "4cbc442b299f4394be984e857b1c9637": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66b4c731c11f4fd3badfb17f2c5fed4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d12f352771742d5a9f4f6ba1fdb28f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ead19d34b6543fd9d054901101f0df9",
      "placeholder": "​",
      "style": "IPY_MODEL_db63a9a43a034cf1b4c5113b011e1f2f",
      "value": " 0/1 [00:00&lt;?, ?example/s]"
     }
    },
    "8ead19d34b6543fd9d054901101f0df9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4b2f6b5744440b89886e2c34a1eca2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b99ccc41a6ff4383bf1781e2fbb2591e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3701922ddbc147af910500d8bb33f6d8",
       "IPY_MODEL_ba5e99343a5e4c0db92a99f14b34a355",
       "IPY_MODEL_7d12f352771742d5a9f4f6ba1fdb28f7"
      ],
      "layout": "IPY_MODEL_eef701e52c0447c093b7461200acde71"
     }
    },
    "ba5e99343a5e4c0db92a99f14b34a355": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66b4c731c11f4fd3badfb17f2c5fed4a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a4b2f6b5744440b89886e2c34a1eca2d",
      "value": 1
     }
    },
    "db63a9a43a034cf1b4c5113b011e1f2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eef701e52c0447c093b7461200acde71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
