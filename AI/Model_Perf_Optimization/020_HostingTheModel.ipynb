{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Hosting the Model\n",
    "\n",
    "**[2.1 Concurrent Model Execution](#2.1-Concurrent-Model-Execution)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.1 Usage Considerations](#2.1.1-Usage-Considerations)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.2 Implementation](#2.1.2-Implementation)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.3 Configure Multiple Instance Groups](#2.1.3-Configure-Multiple-Instance-Groups)<br>\n",
    "**[2.2 Scheduling Strategies](#2.2-Scheduling-Strategies)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.1 Stateless Inference](#2.2.1-Stateless-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.2 Stateful Inference](#2.2.2-Stateful-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.3 Pipelines / Ensembles](#2.2.3-Pipelines-/-Ensembles)<br>\n",
    "**[2.3 Dynamic Batching](#2.3-Dynamic-Batching)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.3.1 Implement Dynamic Batching](#2.3.1-Implement-Dynamic-Batching)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Concurrent Model Execution\n",
    "The Triton architecture allows multiple models and/or multiple instances of the same model to execute in parallel on a single GPU. The following figure shows an example with two models: `model0` and `model1`. Assuming Triton is not currently processing any request, when two requests arrive simultaneously, one for each model, Triton immediately schedules both of them onto the GPU, and the GPUâ€™s hardware scheduler begins working on both computations in parallel. </br>\n",
    "\n",
    "#### Instance Groups\n",
    "By using the *instance-group* setting, the number of execution instances for a model can be increased. The following figure shows model execution when `model1` is configured to allow three execution instances. As shown in the figure, the first three `model1` inference requests are immediately executed in parallel on the GPU. The fourth `model1` inference request must wait until one of the first three executions completes before beginning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Usage Considerations\n",
    "\n",
    "For most models, the Triton feature that provides the largest performance improvement is *dynamic batching*. The key advantages of dynamic batching over setting up multiple instance execution are:\n",
    "- No overhead for model parameter storage\n",
    "- No overhead related to model parameter fetch from the GPU memory\n",
    "- Better utilization of the GPU resources\n",
    "\n",
    "Before we look at the configuration for multiple model execution, let's execute our model again using a single instance, and observe the resource utilization of the GPU. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Launch a terminal window from the JupyterLab launch page.  If you need to open a new launch page, click the '+' icon on the left sidebar menu. You can then use a drag-and-drop action to move the terminal to a sub-window configuration  for better viewing.\n",
    "2. Execute the following command in the terminal before you run the performance tool:<br>\n",
    "\n",
    "```\n",
    "watch -n0.5 nvidia-smi\n",
    "```\n",
    "    You should see an output that resembles:\n",
    "<img src=\"images/NVIDIASMI.png\" style=\"position:relative; left:30px;\" width=800/>\n",
    "\n",
    "3. Execute the same benchmark we used in the previous notebook, but with the batch size reduced to 1, and observe the <code>nvidia-smi</code> output again.  Pay special attention to the memory consumption and GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previous configuration.\n",
    "modelVersion = \"1\"\n",
    "precision = \"fp32\"\n",
    "batchSize = \"8\"\n",
    "maxLatency = \"500\"\n",
    "maxClientThreads = \"10\"\n",
    "maxConcurrency = \"2\"\n",
    "dockerBridge = \"host\"\n",
    "resultsFolderName = \"1\"\n",
    "profilingData = \"utilities/profiling_data_int64\"\n",
    "measurement_request_count = 50\n",
    "percentile_stability = 85\n",
    "stability_percentage = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 1.81311 infer/sec. p85 latency: 8730 usec\n",
      "  Pass [2] throughput: 117.945 infer/sec. p85 latency: 8644 usec\n",
      "  Pass [3] throughput: 116.933 infer/sec. p85 latency: 8741 usec\n",
      "  Pass [4] throughput: 118.937 infer/sec. p85 latency: 8492 usec\n",
      "  Client: \n",
      "    Request count: 354\n",
      "    Throughput: 117.938 infer/sec\n",
      "    Avg client overhead: 0.02%\n",
      "    Avg send request rate: 117.94 infer/sec\n",
      "    p50 latency: 8435 usec\n",
      "    p85 latency: 8660 usec\n",
      "    p90 latency: 8698 usec\n",
      "    p95 latency: 8874 usec\n",
      "    p99 latency: 9007 usec\n",
      "    Avg gRPC time: 8467 usec (marshal 3 usec + response wait 8463 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 354\n",
      "    Execution count: 354\n",
      "    Successful request count: 354\n",
      "    Avg request latency: 8217 usec (overhead 52 usec + queue 17 usec + compute input 20 usec + compute infer 8120 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 119.946 infer/sec. p85 latency: 16705 usec\n",
      "  Pass [2] throughput: 120.942 infer/sec. p85 latency: 16717 usec\n",
      "  Pass [3] throughput: 119.936 infer/sec. p85 latency: 16764 usec\n",
      "  Client: \n",
      "    Request count: 361\n",
      "    Throughput: 120.275 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 120.61 infer/sec\n",
      "    p50 latency: 16630 usec\n",
      "    p85 latency: 16717 usec\n",
      "    p90 latency: 16762 usec\n",
      "    p95 latency: 16778 usec\n",
      "    p99 latency: 16906 usec\n",
      "    Avg gRPC time: 16582 usec (marshal 3 usec + response wait 16578 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 361\n",
      "    Execution count: 361\n",
      "    Successful request count: 361\n",
      "    Avg request latency: 16225 usec (overhead 50 usec + queue 7924 usec + compute input 19 usec + compute infer 8225 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 119.948 infer/sec. p85 latency: 25189 usec\n",
      "  Pass [2] throughput: 119.871 infer/sec. p85 latency: 25119 usec\n",
      "  Pass [3] throughput: 119.943 infer/sec. p85 latency: 25230 usec\n",
      "  Client: \n",
      "    Request count: 360\n",
      "    Throughput: 119.921 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 120.59 infer/sec\n",
      "    p50 latency: 24983 usec\n",
      "    p85 latency: 25160 usec\n",
      "    p90 latency: 25233 usec\n",
      "    p95 latency: 25295 usec\n",
      "    p99 latency: 25577 usec\n",
      "    Avg gRPC time: 24918 usec (marshal 3 usec + response wait 24914 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 361\n",
      "    Execution count: 361\n",
      "    Successful request count: 361\n",
      "    Avg request latency: 24543 usec (overhead 47 usec + queue 16223 usec + compute input 19 usec + compute infer 8247 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 119.94 infer/sec. p85 latency: 33601 usec\n",
      "  Pass [2] throughput: 119.936 infer/sec. p85 latency: 33672 usec\n",
      "  Pass [3] throughput: 118.923 infer/sec. p85 latency: 33619 usec\n",
      "  Client: \n",
      "    Request count: 359\n",
      "    Throughput: 119.6 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 119.93 infer/sec\n",
      "    p50 latency: 33317 usec\n",
      "    p85 latency: 33630 usec\n",
      "    p90 latency: 33713 usec\n",
      "    p95 latency: 33801 usec\n",
      "    p99 latency: 33974 usec\n",
      "    Avg gRPC time: 33288 usec (marshal 3 usec + response wait 33284 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 359\n",
      "    Execution count: 359\n",
      "    Successful request count: 359\n",
      "    Avg request latency: 32841 usec (overhead 48 usec + queue 24508 usec + compute input 19 usec + compute infer 8259 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 119.926 infer/sec. p85 latency: 42059 usec\n",
      "  Pass [2] throughput: 119.933 infer/sec. p85 latency: 42018 usec\n",
      "  Pass [3] throughput: 118.953 infer/sec. p85 latency: 42066 usec\n",
      "  Client: \n",
      "    Request count: 359\n",
      "    Throughput: 119.604 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 119.94 infer/sec\n",
      "    p50 latency: 41825 usec\n",
      "    p85 latency: 42059 usec\n",
      "    p90 latency: 42133 usec\n",
      "    p95 latency: 42224 usec\n",
      "    p99 latency: 42374 usec\n",
      "    Avg gRPC time: 41703 usec (marshal 3 usec + response wait 41699 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 359\n",
      "    Execution count: 359\n",
      "    Successful request count: 359\n",
      "    Avg request latency: 41289 usec (overhead 48 usec + queue 32932 usec + compute input 19 usec + compute infer 8283 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 119.946 infer/sec. p85 latency: 50462 usec\n",
      "  Pass [2] throughput: 118.954 infer/sec. p85 latency: 50545 usec\n",
      "  Pass [3] throughput: 118.944 infer/sec. p85 latency: 50656 usec\n",
      "  Client: \n",
      "    Request count: 358\n",
      "    Throughput: 119.281 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 119.61 infer/sec\n",
      "    p50 latency: 50294 usec\n",
      "    p85 latency: 50593 usec\n",
      "    p90 latency: 50648 usec\n",
      "    p95 latency: 50721 usec\n",
      "    p99 latency: 50958 usec\n",
      "    Avg gRPC time: 50158 usec (marshal 3 usec + response wait 50154 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 358\n",
      "    Execution count: 358\n",
      "    Successful request count: 358\n",
      "    Avg request latency: 49699 usec (overhead 48 usec + queue 41325 usec + compute input 19 usec + compute infer 8300 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 118.958 infer/sec. p85 latency: 59063 usec\n",
      "  Pass [2] throughput: 119.905 infer/sec. p85 latency: 59057 usec\n",
      "  Pass [3] throughput: 118.945 infer/sec. p85 latency: 59122 usec\n",
      "  Client: \n",
      "    Request count: 358\n",
      "    Throughput: 119.269 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 119.60 infer/sec\n",
      "    p50 latency: 58762 usec\n",
      "    p85 latency: 59083 usec\n",
      "    p90 latency: 59201 usec\n",
      "    p95 latency: 59364 usec\n",
      "    p99 latency: 59704 usec\n",
      "    Avg gRPC time: 58629 usec (marshal 3 usec + response wait 58625 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 358\n",
      "    Execution count: 358\n",
      "    Successful request count: 358\n",
      "    Avg request latency: 58185 usec (overhead 49 usec + queue 49794 usec + compute input 19 usec + compute infer 8315 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 118.959 infer/sec. p85 latency: 67614 usec\n",
      "  Pass [2] throughput: 117.944 infer/sec. p85 latency: 67748 usec\n",
      "  Pass [3] throughput: 118.926 infer/sec. p85 latency: 67797 usec\n",
      "  Client: \n",
      "    Request count: 356\n",
      "    Throughput: 118.61 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 118.94 infer/sec\n",
      "    p50 latency: 67289 usec\n",
      "    p85 latency: 67738 usec\n",
      "    p90 latency: 67815 usec\n",
      "    p95 latency: 67972 usec\n",
      "    p99 latency: 68161 usec\n",
      "    Avg gRPC time: 67165 usec (marshal 3 usec + response wait 67161 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 356\n",
      "    Execution count: 356\n",
      "    Successful request count: 356\n",
      "    Avg request latency: 66693 usec (overhead 47 usec + queue 58284 usec + compute input 19 usec + compute infer 8336 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 118.951 infer/sec. p85 latency: 76162 usec\n",
      "  Pass [2] throughput: 118.932 infer/sec. p85 latency: 76289 usec\n",
      "  Pass [3] throughput: 117.947 infer/sec. p85 latency: 76244 usec\n",
      "  Client: \n",
      "    Request count: 356\n",
      "    Throughput: 118.61 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 118.94 infer/sec\n",
      "    p50 latency: 75891 usec\n",
      "    p85 latency: 76228 usec\n",
      "    p90 latency: 76299 usec\n",
      "    p95 latency: 76399 usec\n",
      "    p99 latency: 76544 usec\n",
      "    Avg gRPC time: 75654 usec (marshal 3 usec + response wait 75650 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 356\n",
      "    Execution count: 356\n",
      "    Successful request count: 356\n",
      "    Avg request latency: 75194 usec (overhead 47 usec + queue 66774 usec + compute input 19 usec + compute infer 8347 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 118.949 infer/sec. p85 latency: 84671 usec\n",
      "  Pass [2] throughput: 118.889 infer/sec. p85 latency: 84810 usec\n",
      "  Pass [3] throughput: 117.945 infer/sec. p85 latency: 84856 usec\n",
      "  Client: \n",
      "    Request count: 356\n",
      "    Throughput: 118.595 infer/sec\n",
      "    Avg client overhead: 0.02%\n",
      "    Avg send request rate: 118.93 infer/sec\n",
      "    p50 latency: 84414 usec\n",
      "    p85 latency: 84823 usec\n",
      "    p90 latency: 84857 usec\n",
      "    p95 latency: 84944 usec\n",
      "    p99 latency: 85094 usec\n",
      "    Avg gRPC time: 84165 usec (marshal 3 usec + response wait 84161 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 356\n",
      "    Execution count: 356\n",
      "    Successful request count: 356\n",
      "    Avg request latency: 83688 usec (overhead 49 usec + queue 75257 usec + compute input 19 usec + compute infer 8356 usec + compute output 6 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 117.938 infer/sec, latency 8660 usec\n",
      "Concurrency: 2, throughput: 120.275 infer/sec, latency 16717 usec\n",
      "Concurrency: 3, throughput: 119.921 infer/sec, latency 25160 usec\n",
      "Concurrency: 4, throughput: 119.6 infer/sec, latency 33630 usec\n",
      "Concurrency: 5, throughput: 119.604 infer/sec, latency 42059 usec\n",
      "Concurrency: 6, throughput: 119.281 infer/sec, latency 50593 usec\n",
      "Concurrency: 7, throughput: 119.269 infer/sec, latency 59083 usec\n",
      "Concurrency: 8, throughput: 118.61 infer/sec, latency 67738 usec\n",
      "Concurrency: 9, throughput: 118.61 infer/sec, latency 76228 usec\n",
      "Concurrency: 10, throughput: 118.595 infer/sec, latency 84823 usec\n",
      "CPU times: user 775 ms, sys: 155 ms, total: 930 ms\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Update configuration parameters and run profiler.\n",
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##observed utilization similar to the following:<br/>\n",
    "##expect to observe a major acceleration as a consequence of increasing the number of instance groups?<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Implementation\n",
    "Let's look at how to enable concurrent execution and what impact it will have on our model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-conexec\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-conexec in format onnxruntime_onnx\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  warnings.warn(\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.0135725736618042\n",
      "\n",
      "average L_inf error over output tensors:  0.008322536945343018\n",
      "variance of L_inf error over output tensors:  1.9936597752234775e-05\n",
      "stddev of L_inf error over output tensors:  0.0044650417413765325\n",
      "\n",
      "time of error check of native model:  0.285872220993042 seconds\n",
      "time of error check of onnx model:  6.443050384521484 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 3 root root 4.0K Mar 19 15:33 .\n",
      "drwxr-xr-x 3 root root 4.0K Mar 19 15:33 ..\n",
      "drwxr-xr-x 2 root root 4.0K Mar 19 15:33 1\n",
      "-rw-r--r-- 1 root root  569 Mar 19 15:33 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./candidatemodels/bertQA-onnx-conexec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Configure Multiple Instance Groups\n",
    "In order to specify multiple instances, we need to change the \"count\" value from '1' to a larger number in the `instance_group` section of the \"config.pbtxt\" configuration file. \n",
    "\n",
    "\n",
    "```\n",
    "    instance_group [\n",
    "    {\n",
    "        count: 2\n",
    "        kind: KIND_GPU\n",
    "        gpus: [ 0 ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-conexec/config.pbtxt) in the `bertQA-onnx-conexec` deployment just created to specify two instances of our BERT-based question answering model. You should find the default instance_group block at the end of the file. Change the count variable from 1 to 2.  (see the [solution](solutions/ex-2-1-3_config.pbtxt) as needed)\n",
    "2. To make the comparison fair, also enable TensorRT with the addition of an `execution_accelerators` block inside the `optimization` block:\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "3. Once you have saved your changes (Main menu: File -> Save File), move the model across to Triton by executing the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv ./candidatemodels/bertQA-onnx-conexec model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run our standard stress test against the model. Please compare it to the single instance execution.<br>\n",
    "   Did the throughput change?<br>\n",
    "   Did the latency change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-conexec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 1.52113 infer/sec. p85 latency: 5988 usec\n",
      "  Pass [2] throughput: 167.848 infer/sec. p85 latency: 6165 usec\n",
      "  Pass [3] throughput: 168.861 infer/sec. p85 latency: 6103 usec\n",
      "  Pass [4] throughput: 167.841 infer/sec. p85 latency: 6141 usec\n",
      "  Client: \n",
      "    Request count: 505\n",
      "    Throughput: 168.183 infer/sec\n",
      "    Avg client overhead: 0.06%\n",
      "    Avg send request rate: 168.18 infer/sec\n",
      "    p50 latency: 5895 usec\n",
      "    p85 latency: 6142 usec\n",
      "    p90 latency: 6192 usec\n",
      "    p95 latency: 6286 usec\n",
      "    p99 latency: 6472 usec\n",
      "    Avg gRPC time: 5936 usec (marshal 3 usec + response wait 5932 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 505\n",
      "    Execution count: 505\n",
      "    Successful request count: 505\n",
      "    Avg request latency: 5650 usec (overhead 52 usec + queue 23 usec + compute input 26 usec + compute infer 5542 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 176.941 infer/sec. p85 latency: 11726 usec\n",
      "  Pass [2] throughput: 176.86 infer/sec. p85 latency: 11699 usec\n",
      "  Pass [3] throughput: 175.92 infer/sec. p85 latency: 11681 usec\n",
      "  Client: \n",
      "    Request count: 530\n",
      "    Throughput: 176.574 infer/sec\n",
      "    Avg client overhead: 0.02%\n",
      "    Avg send request rate: 176.91 infer/sec\n",
      "    p50 latency: 11391 usec\n",
      "    p85 latency: 11701 usec\n",
      "    p90 latency: 11919 usec\n",
      "    p95 latency: 12379 usec\n",
      "    p99 latency: 12945 usec\n",
      "    Avg gRPC time: 11311 usec (marshal 3 usec + response wait 11307 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 530\n",
      "    Execution count: 530\n",
      "    Successful request count: 530\n",
      "    Avg request latency: 10890 usec (overhead 45 usec + queue 19 usec + compute input 32 usec + compute infer 10786 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 183.916 infer/sec. p85 latency: 19652 usec\n",
      "  Pass [2] throughput: 184.906 infer/sec. p85 latency: 20015 usec\n",
      "  Pass [3] throughput: 182.919 infer/sec. p85 latency: 19714 usec\n",
      "  Client: \n",
      "    Request count: 552\n",
      "    Throughput: 183.914 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 184.25 infer/sec\n",
      "    p50 latency: 16268 usec\n",
      "    p85 latency: 19813 usec\n",
      "    p90 latency: 20025 usec\n",
      "    p95 latency: 20288 usec\n",
      "    p99 latency: 20654 usec\n",
      "    Avg gRPC time: 16265 usec (marshal 3 usec + response wait 16261 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 552\n",
      "    Execution count: 552\n",
      "    Successful request count: 552\n",
      "    Avg request latency: 15848 usec (overhead 45 usec + queue 4994 usec + compute input 47 usec + compute infer 10754 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 183.923 infer/sec. p85 latency: 22345 usec\n",
      "  Pass [2] throughput: 184.911 infer/sec. p85 latency: 22326 usec\n",
      "  Pass [3] throughput: 186.908 infer/sec. p85 latency: 22109 usec\n",
      "  Client: \n",
      "    Request count: 556\n",
      "    Throughput: 185.248 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 185.58 infer/sec\n",
      "    p50 latency: 21636 usec\n",
      "    p85 latency: 22257 usec\n",
      "    p90 latency: 22432 usec\n",
      "    p95 latency: 22673 usec\n",
      "    p99 latency: 23042 usec\n",
      "    Avg gRPC time: 21559 usec (marshal 3 usec + response wait 21555 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 556\n",
      "    Execution count: 556\n",
      "    Successful request count: 556\n",
      "    Avg request latency: 21117 usec (overhead 47 usec + queue 10324 usec + compute input 50 usec + compute infer 10688 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 186.874 infer/sec. p85 latency: 29793 usec\n",
      "  Pass [2] throughput: 187.9 infer/sec. p85 latency: 29990 usec\n",
      "  Pass [3] throughput: 187.85 infer/sec. p85 latency: 29790 usec\n",
      "  Client: \n",
      "    Request count: 563\n",
      "    Throughput: 187.541 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 187.87 infer/sec\n",
      "    p50 latency: 26517 usec\n",
      "    p85 latency: 29887 usec\n",
      "    p90 latency: 30263 usec\n",
      "    p95 latency: 30713 usec\n",
      "    p99 latency: 31253 usec\n",
      "    Avg gRPC time: 26644 usec (marshal 3 usec + response wait 26640 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 563\n",
      "    Execution count: 563\n",
      "    Successful request count: 563\n",
      "    Avg request latency: 26205 usec (overhead 45 usec + queue 15536 usec + compute input 49 usec + compute infer 10567 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 186.929 infer/sec. p85 latency: 32681 usec\n",
      "  Pass [2] throughput: 187.909 infer/sec. p85 latency: 32804 usec\n",
      "  Pass [3] throughput: 185.909 infer/sec. p85 latency: 32770 usec\n",
      "  Client: \n",
      "    Request count: 561\n",
      "    Throughput: 186.916 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 187.25 infer/sec\n",
      "    p50 latency: 32111 usec\n",
      "    p85 latency: 32720 usec\n",
      "    p90 latency: 32926 usec\n",
      "    p95 latency: 33214 usec\n",
      "    p99 latency: 33920 usec\n",
      "    Avg gRPC time: 32014 usec (marshal 3 usec + response wait 32010 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 561\n",
      "    Execution count: 561\n",
      "    Successful request count: 561\n",
      "    Avg request latency: 31598 usec (overhead 45 usec + queue 20918 usec + compute input 44 usec + compute infer 10583 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 185.871 infer/sec. p85 latency: 41918 usec\n",
      "  Pass [2] throughput: 186.911 infer/sec. p85 latency: 41825 usec\n",
      "  Pass [3] throughput: 185.911 infer/sec. p85 latency: 41874 usec\n",
      "  Client: \n",
      "    Request count: 559\n",
      "    Throughput: 186.231 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 186.56 infer/sec\n",
      "    p50 latency: 37058 usec\n",
      "    p85 latency: 41886 usec\n",
      "    p90 latency: 42094 usec\n",
      "    p95 latency: 42332 usec\n",
      "    p99 latency: 42681 usec\n",
      "    Avg gRPC time: 37511 usec (marshal 3 usec + response wait 37507 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 559\n",
      "    Execution count: 559\n",
      "    Successful request count: 559\n",
      "    Avg request latency: 37121 usec (overhead 46 usec + queue 26392 usec + compute input 36 usec + compute infer 10639 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 185.934 infer/sec. p85 latency: 43711 usec\n",
      "  Pass [2] throughput: 185.908 infer/sec. p85 latency: 43763 usec\n",
      "  Pass [3] throughput: 185.843 infer/sec. p85 latency: 43653 usec\n",
      "  Client: \n",
      "    Request count: 558\n",
      "    Throughput: 185.895 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 186.23 infer/sec\n",
      "    p50 latency: 43019 usec\n",
      "    p85 latency: 43701 usec\n",
      "    p90 latency: 43888 usec\n",
      "    p95 latency: 44103 usec\n",
      "    p99 latency: 44535 usec\n",
      "    Avg gRPC time: 42963 usec (marshal 3 usec + response wait 42959 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 558\n",
      "    Execution count: 558\n",
      "    Successful request count: 558\n",
      "    Avg request latency: 42555 usec (overhead 48 usec + queue 31806 usec + compute input 38 usec + compute infer 10656 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 185.935 infer/sec. p85 latency: 52694 usec\n",
      "  Pass [2] throughput: 183.912 infer/sec. p85 latency: 52669 usec\n",
      "  Pass [3] throughput: 185.913 infer/sec. p85 latency: 52960 usec\n",
      "  Client: \n",
      "    Request count: 556\n",
      "    Throughput: 185.253 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 185.59 infer/sec\n",
      "    p50 latency: 46501 usec\n",
      "    p85 latency: 52750 usec\n",
      "    p90 latency: 53003 usec\n",
      "    p95 latency: 53225 usec\n",
      "    p99 latency: 53754 usec\n",
      "    Avg gRPC time: 48547 usec (marshal 3 usec + response wait 48543 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 556\n",
      "    Execution count: 556\n",
      "    Successful request count: 556\n",
      "    Avg request latency: 48105 usec (overhead 47 usec + queue 37306 usec + compute input 37 usec + compute infer 10708 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 183.928 infer/sec. p85 latency: 54837 usec\n",
      "  Pass [2] throughput: 183.807 infer/sec. p85 latency: 54902 usec\n",
      "  Pass [3] throughput: 184.846 infer/sec. p85 latency: 54991 usec\n",
      "  Client: \n",
      "    Request count: 553\n",
      "    Throughput: 184.194 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 184.53 infer/sec\n",
      "    p50 latency: 54217 usec\n",
      "    p85 latency: 54929 usec\n",
      "    p90 latency: 55089 usec\n",
      "    p95 latency: 55358 usec\n",
      "    p99 latency: 55694 usec\n",
      "    Avg gRPC time: 54115 usec (marshal 3 usec + response wait 54111 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 553\n",
      "    Execution count: 553\n",
      "    Successful request count: 553\n",
      "    Avg request latency: 53681 usec (overhead 47 usec + queue 42850 usec + compute input 39 usec + compute infer 10738 usec + compute output 6 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 168.183 infer/sec, latency 6142 usec\n",
      "Concurrency: 2, throughput: 176.574 infer/sec, latency 11701 usec\n",
      "Concurrency: 3, throughput: 183.914 infer/sec, latency 19813 usec\n",
      "Concurrency: 4, throughput: 185.248 infer/sec, latency 22257 usec\n",
      "Concurrency: 5, throughput: 187.541 infer/sec, latency 29887 usec\n",
      "Concurrency: 6, throughput: 186.916 infer/sec, latency 32720 usec\n",
      "Concurrency: 7, throughput: 186.231 infer/sec, latency 41886 usec\n",
      "Concurrency: 8, throughput: 185.895 infer/sec, latency 43701 usec\n",
      "Concurrency: 9, throughput: 185.253 infer/sec, latency 52750 usec\n",
      "Concurrency: 10, throughput: 184.194 infer/sec, latency 54929 usec\n",
      "CPU times: user 878 ms, sys: 140 ms, total: 1.02 s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-onnx-conexec\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's free up some GPU memory by moving some of the models out of the Triton model repository.  After removing the following three models, only the `bertQA-torchscript` model should remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertQA-torchscript\n"
     ]
    }
   ],
   "source": [
    "# Remove models from the inference server by removing them from the model_repository\n",
    "!mv -f /dli/task/model_repository/bertQA-onnx /dli/task/candidatemodels/\n",
    "!mv -f /dli/task/model_repository/bertQA-onnx-conexec /dli/task/candidatemodels/\n",
    "!mv -f /dli/task/model_repository/bertQA-onnx-trt-fp16 /dli/task/candidatemodels/\n",
    "\n",
    "# List remaining models on the inference server\n",
    "!ls /dli/task/model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Scheduling Strategies\n",
    "Triton supports batch inferencing by allowing individual inference requests to specify a batch of inputs. The inferencing for a batch of inputs is performed at the same time which is especially important for GPUs since it can greatly increase inferencing throughput. In many use cases the individual inference requests are not batched, therefore, they do not benefit from the throughput benefits of batching. <br/>\n",
    "\n",
    "The inference server contains multiple scheduling and batching algorithms that support many different model types and use-cases. The choice of the scheduler / batcher will be driven by several factors the key ones being:\n",
    "- Stateful / stateless nature of your inference workload\n",
    "- Whether your application is composed of models served in isolation or whether a more complex pipeline / ensemble is being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Stateless Inference\n",
    "\n",
    "When dealing with stateless inference (as we are in this class) we have two main options when it comes to scheduling. The first option is the default scheduler which will distribute request to all instances assigned for inference. This is the preferred option when the structure of the inference workload is well understood and where inference will take place at regular batch sizes and time intervals.\n",
    "\n",
    "The second option is dynamic batching which combines individual request and similarly to the default batcher distributes the larges batches across instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Stateful Inference\n",
    "\n",
    "A stateful model (or stateful custom backend) does maintain state between inference requests. The model is expecting multiple inference requests that together form a sequence of inferences that must be routed to the same model instance so that the state being maintained by the model is correctly updated. Moreover, the model may require that Triton provide control signals indicating, for example, sequence start.\n",
    "\n",
    "With the Direct scheduling strategy the sequence batcher ensures not only that all inference requests in a sequence are routed to the same model instance, but also that each sequence is routed to a dedicated batch slot within the model instance. This strategy is required when the model maintains state for each batch slot, and is expecting all inference requests for a given sequence to be routed to the same slot so that the state is correctly updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Pipelines / Ensembles\n",
    "\n",
    "An ensemble model represents a pipeline of one or more models and the connection of input and output tensors between those models. Ensemble models are intended to be used to encapsulate a procedure that involves multiple models, such as \"data preprocessing -> inference -> data post-processing\". Using ensemble models for this purpose can avoid the overhead of transferring intermediate tensors and minimize the number of requests that must be sent to Triton. <br/>\n",
    "\n",
    "The ensemble scheduler must be used for ensemble models, regardless of the scheduler used by the models within the ensemble. With respect to the ensemble scheduler, an ensemble model is not an actual model. Instead, it specifies the data flow between models within the ensemble as Step. The scheduler collects the output tensors in each step, provides them as input tensors for other steps according to the specification. In spite of that, the ensemble model is still viewed as a single model from an external view. we will focus further on one of the most powerful features of Triton, *dynamic batching*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Dynamic Batching\n",
    "Dynamic batching is a feature of Triton that allows inference requests to be combined by the server, so that a batch is created dynamically, resulting in increased throughput.\n",
    "\n",
    "When a model instance becomes available for inferencing, the dynamic batcher will attempt to create batches from the requests that are available in the scheduler. Requests are added to the batch in the order the requests were received. If the dynamic batcher can form a batch of a preferred size(s) it will create a batch of the largest possible preferred size and send it for inferencing. If the dynamic batcher cannot form a batch of a preferred size, it will send a batch of the largest size possible that is less than the max batch size allowed by the model. \n",
    "\n",
    "The dynamic batcher can be configured to allow requests to be delayed for a limited time in the scheduler to allow other requests to join the dynamic batch. For example, the following configuration sets the maximum delay time of 100 microseconds for a request:\n",
    "\n",
    "```\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 4, 8 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Implement Dynamic Batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin again by exporting an ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-dynbatch in format onnxruntime_onnx\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  warnings.warn(\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.0135725736618042\n",
      "\n",
      "average L_inf error over output tensors:  0.008322536945343018\n",
      "variance of L_inf error over output tensors:  1.9936597752234775e-05\n",
      "stddev of L_inf error over output tensors:  0.0044650417413765325\n",
      "\n",
      "time of error check of native model:  0.28313279151916504 seconds\n",
      "time of error check of onnx model:  6.684637546539307 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-trt-dynbatch/config.pbtxt) for dynamic batching using the example snippet. \n",
    "\n",
    "    ```\n",
    "    dynamic_batching {\n",
    "      preferred_batch_size: [ 4, 8 ]\n",
    "      max_queue_delay_microseconds: 100\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "2. Enable TensorRT in the optimization block.\n",
    "\n",
    "    ```\n",
    "    optimization {\n",
    "       execution_accelerators {\n",
    "          gpu_execution_accelerator : [ {\n",
    "             name : \"tensorrt\"\n",
    "             parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "          }]\n",
    "       }\n",
    "    cuda { graphs: 0 }\n",
    "    }\n",
    "    ```\n",
    "3. Once saved, move the model to the Triton model repository and run the performance utility by executing the following cells. ([solution](solutions/ex-2-3-1_config.pbtxt) if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv ./candidatemodels/bertQA-onnx-trt-dynbatch model_repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "...Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 25\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 78\n",
      "    Throughput: 207.874 infer/sec\n",
      "    p50 latency: 38105 usec\n",
      "    p85 latency: 38614 usec\n",
      "    p90 latency: 38739 usec\n",
      "    p95 latency: 39322 usec\n",
      "    p99 latency: 39916 usec\n",
      "    Avg gRPC time: 38201 usec ((un)marshal request/response 6 usec + response wait 38195 usec)\n",
      "  Server: \n",
      "    Inference count: 624\n",
      "    Execution count: 78\n",
      "    Successful request count: 78\n",
      "    Avg request latency: 37768 usec (overhead 53 usec + queue 65 usec + compute input 26 usec + compute infer 37616 usec + compute output 8 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 207.874 infer/sec, latency 38614 usec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 4\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 25\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 169.242 infer/sec\n",
      "    p50 latency: 23636 usec\n",
      "    p85 latency: 23951 usec\n",
      "    p90 latency: 24014 usec\n",
      "    p95 latency: 24110 usec\n",
      "    p99 latency: 24389 usec\n",
      "    Avg gRPC time: 23676 usec ((un)marshal request/response 4 usec + response wait 23672 usec)\n",
      "  Server: \n",
      "    Inference count: 508\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 23317 usec (overhead 55 usec + queue 60 usec + compute input 24 usec + compute infer 23170 usec + compute output 7 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 169.242 infer/sec, latency 23951 usec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 25\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 344\n",
      "    Throughput: 114.6 infer/sec\n",
      "    p50 latency: 8701 usec\n",
      "    p85 latency: 8849 usec\n",
      "    p90 latency: 8893 usec\n",
      "    p95 latency: 8987 usec\n",
      "    p99 latency: 9257 usec\n",
      "    Avg gRPC time: 8717 usec ((un)marshal request/response 4 usec + response wait 8713 usec)\n",
      "  Server: \n",
      "    Inference count: 344\n",
      "    Execution count: 344\n",
      "    Successful request count: 344\n",
      "    Avg request latency: 8366 usec (overhead 56 usec + queue 209 usec + compute input 22 usec + compute infer 8072 usec + compute output 7 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 114.6 infer/sec, latency 8849 usec\n",
      "CPU times: user 1.43 s, sys: 257 ms, total: 1.69 s\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# warm up model with some inferences for faster analysis  (takes about 5 minutes)\n",
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "batchSize = 8\n",
    "!./utilities/run_warmup.sh {modelName} {batchSize}\n",
    "batchSize = 4\n",
    "!./utilities/run_warmup.sh {modelName} {batchSize}\n",
    "batchSize = 1\n",
    "!./utilities/run_warmup.sh {modelName} {batchSize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-dynbatch\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 112.959 infer/sec. p85 latency: 8981 usec\n",
      "  Pass [2] throughput: 114.925 infer/sec. p85 latency: 8881 usec\n",
      "  Pass [3] throughput: 113.947 infer/sec. p85 latency: 8951 usec\n",
      "  Client: \n",
      "    Request count: 342\n",
      "    Throughput: 113.944 infer/sec\n",
      "    Avg client overhead: 0.02%\n",
      "    Avg send request rate: 114.28 infer/sec\n",
      "    p50 latency: 8707 usec\n",
      "    p85 latency: 8936 usec\n",
      "    p90 latency: 8999 usec\n",
      "    p95 latency: 9104 usec\n",
      "    p99 latency: 9638 usec\n",
      "    Avg gRPC time: 8762 usec (marshal 3 usec + response wait 8758 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 342\n",
      "    Execution count: 342\n",
      "    Successful request count: 342\n",
      "    Avg request latency: 8383 usec (overhead 54 usec + queue 203 usec + compute input 61 usec + compute infer 8057 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 119.952 infer/sec. p85 latency: 16849 usec\n",
      "  Pass [2] throughput: 118.944 infer/sec. p85 latency: 16839 usec\n",
      "  Pass [3] throughput: 119.882 infer/sec. p85 latency: 16858 usec\n",
      "  Client: \n",
      "    Request count: 359\n",
      "    Throughput: 119.593 infer/sec\n",
      "    Avg client overhead: 0.02%\n",
      "    Avg send request rate: 119.93 infer/sec\n",
      "    p50 latency: 16717 usec\n",
      "    p85 latency: 16846 usec\n",
      "    p90 latency: 16858 usec\n",
      "    p95 latency: 16892 usec\n",
      "    p99 latency: 17204 usec\n",
      "    Avg gRPC time: 16688 usec (marshal 3 usec + response wait 16684 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 359\n",
      "    Execution count: 359\n",
      "    Successful request count: 359\n",
      "    Avg request latency: 16207 usec (overhead 50 usec + queue 7860 usec + compute input 19 usec + compute infer 8270 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 144.944 infer/sec. p85 latency: 21090 usec\n",
      "  Pass [2] throughput: 144.936 infer/sec. p85 latency: 21183 usec\n",
      "  Pass [3] throughput: 143.932 infer/sec. p85 latency: 21297 usec\n",
      "  Client: \n",
      "    Request count: 434\n",
      "    Throughput: 144.604 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 144.94 infer/sec\n",
      "    p50 latency: 20727 usec\n",
      "    p85 latency: 21173 usec\n",
      "    p90 latency: 21308 usec\n",
      "    p95 latency: 21538 usec\n",
      "    p99 latency: 21778 usec\n",
      "    Avg gRPC time: 20689 usec (marshal 3 usec + response wait 20685 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 434\n",
      "    Execution count: 290\n",
      "    Successful request count: 434\n",
      "    Avg request latency: 20151 usec (overhead 63 usec + queue 9336 usec + compute input 31 usec + compute infer 10712 usec + compute output 8 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 161.93 infer/sec. p85 latency: 24962 usec\n",
      "  Pass [2] throughput: 160.926 infer/sec. p85 latency: 24913 usec\n",
      "  Pass [3] throughput: 160.899 infer/sec. p85 latency: 25093 usec\n",
      "  Client: \n",
      "    Request count: 484\n",
      "    Throughput: 161.252 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 161.58 infer/sec\n",
      "    p50 latency: 24728 usec\n",
      "    p85 latency: 24984 usec\n",
      "    p90 latency: 25099 usec\n",
      "    p95 latency: 25239 usec\n",
      "    p99 latency: 33564 usec\n",
      "    Avg gRPC time: 24741 usec (marshal 3 usec + response wait 24737 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 484\n",
      "    Execution count: 244\n",
      "    Successful request count: 484\n",
      "    Avg request latency: 24208 usec (overhead 69 usec + queue 11917 usec + compute input 36 usec + compute infer 12176 usec + compute output 9 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 165.922 infer/sec. p85 latency: 36768 usec\n",
      "  Pass [2] throughput: 167.887 infer/sec. p85 latency: 30159 usec\n",
      "  Pass [3] throughput: 166.929 infer/sec. p85 latency: 30129 usec\n",
      "  Client: \n",
      "    Request count: 501\n",
      "    Throughput: 166.913 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 167.25 infer/sec\n",
      "    p50 latency: 29742 usec\n",
      "    p85 latency: 30177 usec\n",
      "    p90 latency: 30377 usec\n",
      "    p95 latency: 37454 usec\n",
      "    p99 latency: 41543 usec\n",
      "    Avg gRPC time: 29985 usec (marshal 3 usec + response wait 29981 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 501\n",
      "    Execution count: 209\n",
      "    Successful request count: 501\n",
      "    Avg request latency: 29445 usec (overhead 81 usec + queue 14543 usec + compute input 38 usec + compute infer 14772 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 166.933 infer/sec. p85 latency: 37702 usec\n",
      "  Pass [2] throughput: 165.924 infer/sec. p85 latency: 38034 usec\n",
      "  Pass [3] throughput: 167.898 infer/sec. p85 latency: 42274 usec\n",
      "  Client: \n",
      "    Request count: 501\n",
      "    Throughput: 166.919 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 167.25 infer/sec\n",
      "    p50 latency: 35157 usec\n",
      "    p85 latency: 39234 usec\n",
      "    p90 latency: 42317 usec\n",
      "    p95 latency: 45167 usec\n",
      "    p99 latency: 55366 usec\n",
      "    Avg gRPC time: 35808 usec (marshal 3 usec + response wait 35804 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 501\n",
      "    Execution count: 180\n",
      "    Successful request count: 501\n",
      "    Avg request latency: 35255 usec (overhead 92 usec + queue 17245 usec + compute input 39 usec + compute infer 17867 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 167.934 infer/sec. p85 latency: 47126 usec\n",
      "  Pass [2] throughput: 165.89 infer/sec. p85 latency: 46768 usec\n",
      "  Pass [3] throughput: 167.824 infer/sec. p85 latency: 42095 usec\n",
      "  Client: \n",
      "    Request count: 502\n",
      "    Throughput: 167.216 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 168.22 infer/sec\n",
      "    p50 latency: 41546 usec\n",
      "    p85 latency: 46489 usec\n",
      "    p90 latency: 49339 usec\n",
      "    p95 latency: 58421 usec\n",
      "    p99 latency: 65020 usec\n",
      "    Avg gRPC time: 41763 usec (marshal 3 usec + response wait 41759 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 504\n",
      "    Execution count: 155\n",
      "    Successful request count: 504\n",
      "    Avg request latency: 41186 usec (overhead 106 usec + queue 18765 usec + compute input 43 usec + compute infer 22258 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 168.921 infer/sec. p85 latency: 53633 usec\n",
      "  Pass [2] throughput: 168.908 infer/sec. p85 latency: 47874 usec\n",
      "  Pass [3] throughput: 175.892 infer/sec. p85 latency: 47256 usec\n",
      "  Client: \n",
      "    Request count: 514\n",
      "    Throughput: 171.24 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 171.57 infer/sec\n",
      "    p50 latency: 45956 usec\n",
      "    p85 latency: 48484 usec\n",
      "    p90 latency: 59391 usec\n",
      "    p95 latency: 65235 usec\n",
      "    p99 latency: 72151 usec\n",
      "    Avg gRPC time: 46666 usec (marshal 3 usec + response wait 46662 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 514\n",
      "    Execution count: 138\n",
      "    Successful request count: 514\n",
      "    Avg request latency: 46063 usec (overhead 122 usec + queue 21274 usec + compute input 45 usec + compute infer 24607 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 171.938 infer/sec. p85 latency: 53666 usec\n",
      "  Pass [2] throughput: 168.922 infer/sec. p85 latency: 53149 usec\n",
      "  Pass [3] throughput: 173.89 infer/sec. p85 latency: 53638 usec\n",
      "  Client: \n",
      "    Request count: 515\n",
      "    Throughput: 171.584 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 171.92 infer/sec\n",
      "    p50 latency: 52414 usec\n",
      "    p85 latency: 53539 usec\n",
      "    p90 latency: 62193 usec\n",
      "    p95 latency: 71701 usec\n",
      "    p99 latency: 79151 usec\n",
      "    Avg gRPC time: 52342 usec (marshal 3 usec + response wait 52338 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 515\n",
      "    Execution count: 121\n",
      "    Successful request count: 515\n",
      "    Avg request latency: 51753 usec (overhead 136 usec + queue 22344 usec + compute input 48 usec + compute infer 29207 usec + compute output 17 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 174.921 infer/sec. p85 latency: 58939 usec\n",
      "  Pass [2] throughput: 177.88 infer/sec. p85 latency: 55676 usec\n",
      "  Pass [3] throughput: 171.913 infer/sec. p85 latency: 58783 usec\n",
      "  Client: \n",
      "    Request count: 525\n",
      "    Throughput: 174.905 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 175.24 infer/sec\n",
      "    p50 latency: 55052 usec\n",
      "    p85 latency: 58477 usec\n",
      "    p90 latency: 59057 usec\n",
      "    p95 latency: 74403 usec\n",
      "    p99 latency: 90156 usec\n",
      "    Avg gRPC time: 56509 usec (marshal 3 usec + response wait 56505 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 525\n",
      "    Execution count: 110\n",
      "    Successful request count: 525\n",
      "    Avg request latency: 55926 usec (overhead 145 usec + queue 23490 usec + compute input 51 usec + compute infer 32221 usec + compute output 19 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 113.944 infer/sec, latency 8936 usec\n",
      "Concurrency: 2, throughput: 119.593 infer/sec, latency 16846 usec\n",
      "Concurrency: 3, throughput: 144.604 infer/sec, latency 21173 usec\n",
      "Concurrency: 4, throughput: 161.252 infer/sec, latency 24984 usec\n",
      "Concurrency: 5, throughput: 166.913 infer/sec, latency 30177 usec\n",
      "Concurrency: 6, throughput: 166.919 infer/sec, latency 39234 usec\n",
      "Concurrency: 7, throughput: 167.216 infer/sec, latency 46489 usec\n",
      "Concurrency: 8, throughput: 171.24 infer/sec, latency 48484 usec\n",
      "Concurrency: 9, throughput: 171.584 infer/sec, latency 53539 usec\n",
      "Concurrency: 10, throughput: 174.905 infer/sec, latency 58477 usec\n",
      "CPU times: user 289 ms, sys: 58.4 ms, total: 347 ms\n",
      "Wall time: 40.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "maxConcurency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observed a fairly dramatic improvement in both latency and throughput. \n",
    "* What do you think was bottlenecking the multiple instance implementation?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
