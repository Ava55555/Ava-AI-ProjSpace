{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Server Performance\n",
    "\n",
    "**[3.1 Assessing the impact of Optimizations](#3.1-Assessing-the-impact-of-Optimizations)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [3.1.1 Profile the Model](#3.1.1-Exercise:-Profile-the-Model)<br>\n",
    "**[3.2 Monitoring and Responding to Performance Fluctuations](#3.2-Monitoring-and-Responding-to-Performance-Fluctuations)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [3.2.1 Viewing Prometheus Metrics](#3.2.1-Viewing-Prometheus-Metrics)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [3.2.2 Interpreting the Metrics](#3.2.2-Interpreting-the-Metrics)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Assessing the impact of Optimizations\n",
    "The performance tool that we've been using has an additional feature: not only does it display the results on the screen, it also saves the data in a tabular format to the following location: \n",
    "\n",
    "<code>\"./results/${MODEL_NAME}/results${RESULTS_ID}_${TIMESTAMP}.csv\"</code>\n",
    "\n",
    "To assess the impact of the various optimizations, let's take advantage of the previously generated log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Profile the Model\n",
    "We executed <code>bertQA-torchscript</code> as well as <code>bertQA-onnx-trt-dynbatch</code> earlier, so we should already have the logs from that execution saved. Let's look at the content of the appropriate log folders. If you have executed the performance tool more than once, you might see multiple log files with different time stamps created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/bertQA-torchscript/results_1_240319_1505.csv\n",
      "./results/bertQA-onnx-trt-dynbatch/results_1_240319_1554.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ./results/bertQA-torchscript/results_1*\n",
    "!ls ./results/bertQA-onnx-trt-dynbatch/results_1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Monitoring and Responding to Performance Fluctuations\n",
    "\n",
    "Understanding the performance of your inference server is not only critical at the initial planning stage but equally important throughout the lifetime of the application. The ability to capture metrics describing server performance is not only central to the ability to respond to issues, but also is a foundation of more advanced features like automatic scaling.  The diagram below demonstrates a simplified view of the Triton deployment architecture. With [Kubernetes], create a configuration that will automatically scale with the increased demand within your data center or, if necessary, burst the excess workload to the cloud/clouds. <br/>\n",
    "\n",
    "<img width=700 src=\"images/DeploymentArchitecture.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Viewing Prometheus Metrics\n",
    "Triton exposes [Prometheus](https://prometheus.io/) performance metrics for monitoring on port 8002 by default. These include metrics on GPU power usage, GPU memory, request counts, and latency measures. let's query the metrics captured throughout our performance runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.20.0.3:8002...\n",
      "* Connected to triton (172.20.0.3) port 8002 (#0)\n",
      "> GET /metrics HTTP/1.1\n",
      "> Host: triton:8002\n",
      "> User-Agent: curl/7.81.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: text/plain; charset=utf-8\n",
      "< Content-Length: 5321\n",
      "< \n",
      "# HELP nv_inference_request_success Number of successful inference requests, all batch sizes\n",
      "# TYPE nv_inference_request_success counter\n",
      "nv_inference_request_success{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-onnx-trt-dynbatch\",version=\"1\"} 5460\n",
      "nv_inference_request_success{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-torchscript\",version=\"1\"} 323\n",
      "# HELP nv_inference_request_failure Number of failed inference requests, all batch sizes\n",
      "# TYPE nv_inference_request_failure counter\n",
      "nv_inference_request_failure{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-onnx-trt-dynbatch\",version=\"1\"} 0\n",
      "nv_inference_request_failure{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-torchscript\",version=\"1\"} 0\n",
      "# HELP nv_inference_count Number of inferences performed (does not include cached requests)\n",
      "# TYPE nv_inference_count counter\n",
      "nv_inference_count{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-onnx-trt-dynbatch\",version=\"1\"} 6874\n",
      "nv_inference_count{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-torchscript\",version=\"1\"} 2584\n",
      "# HELP nv_inference_exec_count Number of model executions performed (does not include cached requests)\n",
      "# TYPE nv_inference_exec_count counter\n",
      "nv_inference_exec_count{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-onnx-trt-dynbatch\",version=\"1\"} 2921\n",
      "nv_inference_exec_count{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-torchscript\",version=\"1\"} 323\n",
      "# HELP nv_inference_request_duration_us Cumulative inference request duration in microseconds (includes cached requests)\n",
      "# TYPE nv_inference_request_duration_us counter\n",
      "nv_inference_request_duration_us{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-onnx-trt-dynbatch\",version=\"1\"} 358329136\n",
      "nv_inference_request_duration_us{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-torchscript\",version=\"1\"} 102239352\n",
      "# HELP nv_inference_queue_duration_us Cumulative inference queuing duration in microseconds (includes cached requests)\n",
      "# TYPE nv_inference_queue_duration_us counter\n",
      "nv_inference_queue_duration_us{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-onnx-trt-dynbatch\",version=\"1\"} 73170325\n",
      "nv_inference_queue_duration_us{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-torchscript\",version=\"1\"} 33010667\n",
      "# HELP nv_inference_compute_input_duration_us Cumulative compute input duration in microseconds (does not include cached requests)\n",
      "# TYPE nv_inference_compute_input_duration_us counter\n",
      "nv_inference_compute_input_duration_us{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-onnx-trt-dynbatch\",version=\"1\"} 211889\n",
      "nv_inference_compute_input_duration_us{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-torchscript\",version=\"1\"} 14617\n",
      "# HELP nv_inference_compute_infer_duration_us Cumulative compute inference duration in microseconds (does not include cached requests)\n",
      "# TYPE nv_inference_compute_infer_duration_us counter\n",
      "nv_inference_compute_infer_duration_us{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-onnx-trt-dynbatch\",version=\"1\"} 284390197\n",
      "nv_inference_compute_infer_duration_us{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-torchscript\",version=\"1\"} 69169508\n",
      "# HELP nv_inference_compute_output_duration_us Cumulative inference compute output duration in microseconds (does not include cached requests)\n",
      "# TYPE nv_inference_compute_output_duration_us counter\n",
      "nv_inference_compute_output_duration_us{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-onnx-trt-dynbatch\",version=\"1\"} 61260\n",
      "nv_inference_compute_output_duration_us{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\",model=\"bertQA-torchscript\",version=\"1\"} 17515\n",
      "# HELP nv_energy_consumption GPU energy consumption in joules since the Triton Server started\n",
      "# TYPE nv_energy_consumption counter\n",
      "nv_energy_consumption{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\"} 0\n",
      "# HELP nv_gpu_utilization GPU utilization rate [0.0 - 1.0)\n",
      "# TYPE nv_gpu_utilization gauge\n",
      "nv_gpu_utilization{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\"} 0\n",
      "# HELP nv_gpu_memory_total_bytes GPU total memory, in bytes\n",
      "# TYPE nv_gpu_memory_total_bytes gauge\n",
      "nv_gpu_memory_total_bytes{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\"} 25702694912\n",
      "# HELP nv_gpu_memory_used_bytes GPU used memory, in bytes\n",
      "# TYPE nv_gpu_memory_used_bytes gauge\n",
      "nv_gpu_memory_used_bytes{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\"} 4714397696\n",
      "# HELP nv_gpu_power_usage GPU power usage in watts\n",
      "# TYPE nv_gpu_power_usage gauge\n",
      "nv_gpu_power_usage{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\"} 0\n",
      "# HELP nv_gpu_power_limit GPU power management limit in watts\n",
      "# TYPE nv_gpu_power_limit gauge\n",
      "nv_gpu_power_limit{gpu_uuid=\"GPU-adf51400-e5c2-11ee-bd50-8f821a947b31\"} 0\n",
      "# HELP nv_cpu_utilization CPU utilization rate [0.0 - 1.0]\n",
      "# TYPE nv_cpu_utilization gauge\n",
      "nv_cpu_utilization 0.001941209095951192\n",
      "# HELP nv_cpu_memory_total_bytes CPU total memory (RAM), in bytes\n",
      "# TYPE nv_cpu_memory_total_bytes gauge\n",
      "nv_cpu_memory_total_bytes 464934928384\n",
      "# HELP nv_cpu_memory_used_bytes CPU used memory (RAM), in bytes\n",
      "# TYPE nv_cpu_memory_used_bytes gauge\n",
      "nv_cpu_memory_used_bytes 47536640000\n",
      "* Connection #0 to host triton left intact\n"
     ]
    }
   ],
   "source": [
    "# Use a curl command to request the metrics\n",
    "prometheus_url = tritonServerHostName + \":8002/metrics\"\n",
    "!curl -v {prometheus_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Interpreting the Metrics\n",
    "The Prometheus metrics output is a list of metrics, where each is provided with the form:\n",
    "\n",
    "```\n",
    "# HELP <metric_name and description>\n",
    "# TYPE <metric_name and type>\n",
    "metric_name{gpu_uuid=\"GPU-xxxxxx\",...} <data>\n",
    "```\n",
    "\n",
    "For example, if the inference server models includes two models, you should see among the list some metrics that are specific to each model, and other metrics that are more general about the GPU they both share.<br>\n",
    "\n",
    "#### Count Example\n",
    "The following example indicates that the inference count for the `bertQA-onnx-trt-dynbatch` model is 10,105 so far, while the inference count for `bertQA-torchscript` model is 717.<br>What do your results show?\n",
    "```\n",
    "# HELP nv_inference_count Number of inferences performed\n",
    "# TYPE nv_inference_count counter\n",
    "nv_inference_count{gpu_uuid=\"GPU-640c6e00-43dd-9fae-9f9a-cb6af82df8e9\",model=\"bertQA-onnx-trt-dynbatch\",version=\"1\"} 10105.000000\n",
    "nv_inference_count{gpu_uuid=\"GPU-640c6e00-43dd-9fae-9f9a-cb6af82df8e9\",model=\"bertQA-torchscript\",version=\"1\"} 717.000000\n",
    "```\n",
    "\n",
    "#### GPU Power Example\n",
    "The following example indicates that current GPU power usage is about 40 watts.<br>What do your results show?\n",
    "```\n",
    "# HELP nv_gpu_power_usage GPU power usage in watts\n",
    "# TYPE nv_gpu_power_usage gauge\n",
    "nv_gpu_power_usage{gpu_uuid=\"GPU-640c6e00-43dd-9fae-9f9a-cb6af82df8e9\"} 39.958000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What Do Your Results Indicate?\n",
    "\n",
    "* Can you identify the current utilization rate? \n",
    "* Why is it zero? \n",
    "* How much memory are we using? \n",
    "* Why do you think we are using the GPU memory even though there are no requests executed against our server? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully configured optimizations and profiled the model.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
