{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Exporting the Model\n",
    "Exporting a BERT checkpoint trained using PyTorch\n",
    "\n",
    "**[1.1 Overview: Optimization and Performance](#1.1-Overview:-Optimization-and-Performance)<br>**\n",
    "**[1.2 Export a BERT Checkpoint](#1.2-Export-a-BERT-Checkpoint)<br>**\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.2.1 Triton Model Repository](#1.2.1-Triton-Model-Repository)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.2.2 TorchScript Export](#1.2.2-TorchScript-Export)<br>\n",
    "**[1.3 Test Our Export](#1.3-Test-Our-Export)<br>**\n",
    "**[1.4 Beyond TorchScript](#1.4-Beyond-TorchScript)<br>**\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.4.1 : Enable TensorRT Optimization](#1.4.1-:-Enable-TensorRT-Optimization)<br>\n",
    "**[1.5 Performance Comparison](#1.5-Performance-Comparison)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Overview: Optimization and Performance\n",
    "Optimization of the trained model will have a fairly dramatic impact on the inference performance, measured in bandwidth and latency. \n",
    "Modern inference servers typically support substantially more than one model format to cater to a wider range of projects, tools, and preferences. Since in this class we are working with a BERT checkpoint trained using PyTorch, and we are deploying it with Triton Inference Server, we will focus on options for deploying PyTorch-based models. These include:\n",
    "   - PyTorch JIT / TorchScript\n",
    "   - ONNX runtime\n",
    "   - ONNX-TensorRT\n",
    "   - TensorRT\n",
    "    \n",
    "It's important to point out that Triton Server supports a much broader set of deployment mechanisms including:\n",
    "   - TensorFlow GraphDef\n",
    "   - TensorFlow saved model\n",
    "   - Caffe 2 exports\n",
    "   - Custom models (which can be any custom executable)\n",
    "\n",
    "In this section we will look at how to deploy a model using some of the deployment engines listed above and the impact each has on performance. We will also experiment with some of the key settings, namely the batch size and numerical precision (FP32 and FP16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Export a BERT Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT model checkpoint we want to deploy, <code>bert_qa.pt</code>, should be located in your `data` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/bert_qa.pt\n"
     ]
    }
   ],
   "source": [
    "!ls data/*.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is a standard checkpoint of a BERT-Large network, fine-tuned on the [Stanford Question Answering Dataset (SQuAD)](https://arxiv.org/abs/1606.05250). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Scripts\n",
    "As we explore various deployment configurations, we'll repeat some steps over and over.  Therefore, we'll use some helper scripts to partially automate the process so that we can focus our attention on the configuration settings and results.  You can explore the code details yourself if you are curious:\n",
    "\n",
    "- [utilities/wait_for_triton_server.sh](utilities/wait_for_triton_server.sh): Check the \"live\" and \"ready\" status of the Triton server via the API\n",
    "- [deployer/deployer.py](deployer/deployer.py): Convert a checkpoint to a deployable model and export it\n",
    "- [utilities/run_perf_analyzer_local.sh](utilities/run_perf_analyzer_local.sh): Measure performance with the [perf_analyzer](https://github.com/triton-inference-server/client/blob/main/src/c++/perf_analyzer/README.md) application\n",
    "- [utilities/run_warmup.sh](utilities/run_warmup.sh): Run some inferences using `perf_analyzer` to warm up the model.  Prewarming the model results in more stable measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Triton server has been deployed in a container and is available to us at host \"triton\" on port \"8000\". Run the next cell to to check for a \"200 OK\" HTTP response from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 Triton Model Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "<model-repository-path>/\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    [<output-labels-file> ...]\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    ...\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    [<output-labels-file> ...]\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    ...\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab container is configured to use the <code>./model_repository</code> folder as the model repository, so any change within this folder will affect the behavior of Triton Server.<br/>\n",
    "\n",
    "In order to expose a new model to Triton you need to: <br/>\n",
    "   1. Create a new model folder in the model repository. The name of the folder needs to reflect the name of the service you will be exposing to your users/applications.<br/>\n",
    "   2. Within the model folder, create a <code>config.pbtxt</code> file that contains the basic serving configuration for the model<br/>\n",
    "   3. Also within the model folder, create at least one folder containing a copy of the model. The name of the folder reflects the version name of the model. You can create and host multiple versions of the same model.<br/>\n",
    "    \n",
    "Next, we'll walk through the process of exporting the model to Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 TorchScript Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelName = \"bertQA-torchscript\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-torchscript in format pytorch_libtorch\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_recursive.py:260: UserWarning: 'bias' was found in ScriptModule constants,  but it is a non-constant parameter. Consider removing it.\n",
      "  warnings.warn(\"'{}' was found in ScriptModule constants, \"\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.02516031265258789\n",
      "\n",
      "average L_inf error over output tensors:  0.013570129871368408\n",
      "variance of L_inf error over output tensors:  0.00011137690313015962\n",
      "stddev of L_inf error over output tensors:  0.010553525625598283\n",
      "\n",
      "time of error check of native model:  0.5635364055633545 seconds\n",
      "time of error check of ts model:  0.9709277153015137 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --ts-script \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint \"/dli/task/data/bert_qa.pt\" \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `deployer.py` script loads the `bert_qa.pt` checkpoint, deploys it in `ts-script` format into a folder called `bertQA-torchscript`, and marks it as version `1`. We will discuss some of the more advanced settings later. For now, let's inspect the files generated by the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Mar 19 15:05 .\n",
      "drwxr-xr-x 3 root root 4096 Mar 19 15:05 ..\n",
      "drwxr-xr-x 2 root root 4096 Mar 19 15:05 1\n",
      "-rw-r--r-- 1 root root  568 Mar 19 15:05 config.pbtxt\n",
      "total 1309296\n",
      "drwxr-xr-x 2 root root       4096 Mar 19 15:05 .\n",
      "drwxr-xr-x 3 root root       4096 Mar 19 15:05 ..\n",
      "-rw-r--r-- 1 root root 1340709950 Mar 19 15:05 model.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-torchscript/\n",
    "!ls -al ./candidatemodels/bertQA-torchscript/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the script exported the model into the TorchScript format and saved it as `model.pt`. It also generated the `config.pbtxt` file. <br> \n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"bertQA-torchscript\"\n",
      "platform: \"pytorch_libtorch\"\n",
      "max_batch_size: 8\n",
      "input [\n",
      "{\n",
      "    name: \"input__0\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "},\n",
      "{\n",
      "    name: \"input__1\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "},\n",
      "{\n",
      "    name: \"input__2\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "}\n",
      "]\n",
      "output [\n",
      "{\n",
      "    name: \"output__0\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [384]\n",
      "}, \n",
      "{\n",
      "    name: \"output__1\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [384]\n",
      "}\n",
      "]\n",
      "optimization {\n",
      "  cuda {\n",
      "    graphs: 0\n",
      "  }\n",
      "}\n",
      "instance_group [\n",
      "    {\n",
      "        count: 1\n",
      "        kind: KIND_GPU\n",
      "        gpus: [ 0 ]\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "!cat ./candidatemodels/bertQA-torchscript/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is fairly simple and defines:\n",
    "   - Name of the model\n",
    "   - Type of platform to be used for inference; in this case `pytorch_libtorch`\n",
    "   - Input and output dimensions used by the network\n",
    "   - Optimizations used; in this case GPU and the default TorchScript optimization \n",
    "   - Instance group configuration; in this case instance group count is set to one, meaning that only one copy of the model will be held in GPU memory (GPU 0 is being used).\n",
    "    \n",
    "To deploy the model, move the folder to the Triton model repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-torchscript model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully deployed your first model to Triton Inference Server!\n",
    "let's see how our model is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1.3 Test Our Export\n",
    "Execute the cells below to start an inference process and make a simple measurement of inference performance. First, we'll set up some configuration. `maxConcurrency` is set to two, meaning that the stress test will be executed twice. The first run will use just a single thread and the second one will use two threads to query the server. Without turning on the concurrent model execution or dynamic batching features, what do you think will be the impact on performance of running two processes querying the server? Do you think:<br/>\n",
    "- Bandwidth will increase or decrease?<br/>\n",
    "- Latency will increase or decrease?<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelVersion = \"1\"\n",
    "precision = \"fp32\"\n",
    "batchSize = \"8\"\n",
    "maxLatency = \"500\"\n",
    "maxClientThreads = \"10\"\n",
    "maxConcurrency = \"2\"\n",
    "dockerBridge = \"host\"\n",
    "resultsFolderName = \"1\"\n",
    "profilingData = \"utilities/profiling_data_int64\"\n",
    "measurement_request_count = 50\n",
    "percentile_stability = 85\n",
    "stability_percentage = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-torchscript\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 2 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 30.2815 infer/sec. p85 latency: 206343 usec\n",
      "  Pass [2] throughput: 39.2656 infer/sec. p85 latency: 206396 usec\n",
      "  Pass [3] throughput: 39.266 infer/sec. p85 latency: 205569 usec\n",
      "  Client: \n",
      "    Request count: 161\n",
      "    Throughput: 35.772 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 4.56 infer/sec\n",
      "    p50 latency: 204626 usec\n",
      "    p85 latency: 206283 usec\n",
      "    p90 latency: 206475 usec\n",
      "    p95 latency: 206657 usec\n",
      "    p99 latency: 218155 usec\n",
      "    Avg gRPC time: 223111 usec (marshal 6 usec + response wait 223104 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1288\n",
      "    Execution count: 161\n",
      "    Successful request count: 161\n",
      "    Avg request latency: 222742 usec (overhead 83 usec + queue 24 usec + compute input 46 usec + compute infer 222534 usec + compute output 54 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 38.5393 infer/sec. p85 latency: 411540 usec\n",
      "  Pass [2] throughput: 39.2649 infer/sec. p85 latency: 413784 usec\n",
      "  Pass [3] throughput: 38.5384 infer/sec. p85 latency: 414599 usec\n",
      "  Client: \n",
      "    Request count: 160\n",
      "    Throughput: 38.7809 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 4.88 infer/sec\n",
      "    p50 latency: 411899 usec\n",
      "    p85 latency: 413811 usec\n",
      "    p90 latency: 413963 usec\n",
      "    p95 latency: 414599 usec\n",
      "    p99 latency: 415303 usec\n",
      "    Avg gRPC time: 410210 usec (marshal 6 usec + response wait 410203 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1280\n",
      "    Execution count: 160\n",
      "    Successful request count: 160\n",
      "    Avg request latency: 409717 usec (overhead 81 usec + queue 203724 usec + compute input 44 usec + compute infer 205812 usec + compute output 55 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 35.772 infer/sec, latency 206283 usec\n",
      "Concurrency: 2, throughput: 38.7809 infer/sec, latency 413811 usec\n",
      "CPU times: user 503 ms, sys: 125 ms, total: 628 ms\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-torchscript\"\n",
    "maxConcurrency = \"2\"\n",
    "batchSize = \"8\"\n",
    "print(\"Running: \"+modelName)\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went okay you should have been presented with output similar to the following example result, showing the inference performance across two different configurations.<br/>\n",
    "<img src=\"images/InferenceJob1.png\" alt=\"Example output of inference job 1\" style=\"width: 1200px;\"/>\n",
    "\n",
    "If you happened to get \"error: failed to get model metatdata\", try running the cell again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Beyond TorchScript\n",
    "\n",
    "Let's investigate a different route for model deployment onto Triton, namely <a href=\"https://onnx.ai\">Open Neural Network Exchange (ONNX)</a>. ONNX is an open format for representation and exchange of neural network models. It defines a common set of operators that are used to build common models, as well as a file format for exchanging them. The advantage of ONNX is that it is relatively widely adopted and can be used to exchange models between <a href=\"https://onnx.ai/supported-tools.html\">a wide range of deep learning tools</a>, such as deep learning frameworks or deployment tools. This also includes TensorRT, which can consume ONNX models. </br>\n",
    "\n",
    "As before, start by exporting the model, but this time using the ONNX format. We will take advantage of the export tool that we used earlier, but change the export format from <code>ts-script</code> to <code>onnx</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx in format onnxruntime_onnx\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  warnings.warn(\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.0135725736618042\n",
      "\n",
      "average L_inf error over output tensors:  0.008322536945343018\n",
      "variance of L_inf error over output tensors:  1.9936597752234775e-05\n",
      "stddev of L_inf error over output tensors:  0.0044650417413765325\n",
      "\n",
      "time of error check of native model:  0.28491973876953125 seconds\n",
      "time of error check of onnx model:  6.45402717590332 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/serialization.md\">TorchScript serialization format</a>, the <a href=\"https://onnx.ai/get-started.html\">ONNX format</a> can be inspected quite easily (and parts are human readable). Lets have a look at the assets our export has generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Mar 19 15:08 .\n",
      "drwxr-xr-x 3 root root 4096 Mar 19 15:07 ..\n",
      "drwxr-xr-x 2 root root 4096 Mar 19 15:07 1\n",
      "-rw-r--r-- 1 root root  561 Mar 19 15:08 config.pbtxt\n",
      "total 1305596\n",
      "drwxr-xr-x 2 root root       4096 Mar 19 15:07 .\n",
      "drwxr-xr-x 3 root root       4096 Mar 19 15:08 ..\n",
      "-rw-r--r-- 1 root root 1336922015 Mar 19 15:07 model.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-onnx/\n",
    "!ls -al ./candidatemodels/bertQA-onnx/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we have a configuration file as well as a model, this time stored in ONNX format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple of options for executing the ONNX-based export in Triton:\n",
    "- We can take advantage of ONNX runtime </br>\n",
    "- We can ask TensorRT to parse the ONNX assets in order to generate a TensorRT engine to use instead </br>\n",
    "\n",
    "We'll try both approaches and look at the impact this has on inference performance. In order to deploy the current ONNX model, move it to the model repository..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and run our stress testing code across 10 different levels of concurrency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 70.6581 infer/sec. p85 latency: 113645 usec\n",
      "  Pass [2] throughput: 70.6537 infer/sec. p85 latency: 112593 usec\n",
      "  Pass [3] throughput: 71.9854 infer/sec. p85 latency: 112893 usec\n",
      "  Client: \n",
      "    Request count: 160\n",
      "    Throughput: 71.0991 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.94 infer/sec\n",
      "    p50 latency: 112372 usec\n",
      "    p85 latency: 112886 usec\n",
      "    p90 latency: 113174 usec\n",
      "    p95 latency: 113645 usec\n",
      "    p99 latency: 115396 usec\n",
      "    Avg gRPC time: 112482 usec (marshal 5 usec + response wait 112476 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1280\n",
      "    Execution count: 160\n",
      "    Successful request count: 160\n",
      "    Avg request latency: 112209 usec (overhead 51 usec + queue 17 usec + compute input 119 usec + compute infer 112012 usec + compute output 9 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 70.6564 infer/sec. p85 latency: 224214 usec\n",
      "  Pass [2] throughput: 71.9874 infer/sec. p85 latency: 224737 usec\n",
      "  Pass [3] throughput: 70.6545 infer/sec. p85 latency: 225387 usec\n",
      "  Client: \n",
      "    Request count: 160\n",
      "    Throughput: 71.0994 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.94 infer/sec\n",
      "    p50 latency: 224181 usec\n",
      "    p85 latency: 225073 usec\n",
      "    p90 latency: 225182 usec\n",
      "    p95 latency: 225387 usec\n",
      "    p99 latency: 225681 usec\n",
      "    Avg gRPC time: 223357 usec (marshal 5 usec + response wait 223351 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1280\n",
      "    Execution count: 160\n",
      "    Successful request count: 160\n",
      "    Avg request latency: 223041 usec (overhead 50 usec + queue 111003 usec + compute input 26 usec + compute infer 111952 usec + compute output 9 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 70.6568 infer/sec. p85 latency: 339214 usec\n",
      "  Pass [2] throughput: 71.987 infer/sec. p85 latency: 339903 usec\n",
      "  Pass [3] throughput: 70.6557 infer/sec. p85 latency: 340057 usec\n",
      "  Client: \n",
      "    Request count: 160\n",
      "    Throughput: 71.0999 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.94 infer/sec\n",
      "    p50 latency: 339157 usec\n",
      "    p85 latency: 339901 usec\n",
      "    p90 latency: 340052 usec\n",
      "    p95 latency: 340264 usec\n",
      "    p99 latency: 340493 usec\n",
      "    Avg gRPC time: 337062 usec (marshal 6 usec + response wait 337055 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1280\n",
      "    Execution count: 160\n",
      "    Successful request count: 160\n",
      "    Avg request latency: 336698 usec (overhead 49 usec + queue 223713 usec + compute input 28 usec + compute infer 112898 usec + compute output 9 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 69.3234 infer/sec. p85 latency: 454335 usec\n",
      "  Pass [2] throughput: 70.6543 infer/sec. p85 latency: 455159 usec\n",
      "  Pass [3] throughput: 70.6552 infer/sec. p85 latency: 455673 usec\n",
      "  Client: \n",
      "    Request count: 158\n",
      "    Throughput: 70.211 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.83 infer/sec\n",
      "    p50 latency: 454603 usec\n",
      "    p85 latency: 455443 usec\n",
      "    p90 latency: 455569 usec\n",
      "    p95 latency: 455886 usec\n",
      "    p99 latency: 456114 usec\n",
      "    Avg gRPC time: 452337 usec (marshal 6 usec + response wait 452330 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1264\n",
      "    Execution count: 158\n",
      "    Successful request count: 158\n",
      "    Avg request latency: 451960 usec (overhead 50 usec + queue 338335 usec + compute input 29 usec + compute infer 113536 usec + compute output 9 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 70.6566 infer/sec. p85 latency: 571082 usec\n",
      "  Pass [2] throughput: 69.3212 infer/sec. p85 latency: 571742 usec\n",
      "  Pass [3] throughput: 70.6545 infer/sec. p85 latency: 572240 usec\n",
      "Measured latency went over the set limit of 500 msec. \n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 71.0991 infer/sec, latency 112886 usec\n",
      "Concurrency: 2, throughput: 71.0994 infer/sec, latency 225073 usec\n",
      "Concurrency: 3, throughput: 71.0999 infer/sec, latency 339901 usec\n",
      "Concurrency: 4, throughput: 70.211 infer/sec, latency 455443 usec\n",
      "CPU times: user 581 ms, sys: 221 ms, total: 802 ms\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-onnx\"\n",
    "maxConcurrency = \"10\"\n",
    "batchSize = \"8\"\n",
    "print(\"Running: \"+modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the results. Did we manage to run our benchmark at all 10 concurrency levels (or did the benchmark time out earlier)? What happened to the request latency in relation to the 500 ms time limit we configured?</br>\n",
    "\n",
    "Now let's export the ONNX model again, so that we can configure it for TensorRT execution.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-fp16 in format onnxruntime_onnx\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  warnings.warn(\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.0135725736618042\n",
      "\n",
      "average L_inf error over output tensors:  0.008322536945343018\n",
      "variance of L_inf error over output tensors:  1.9936597752234775e-05\n",
      "stddev of L_inf error over output tensors:  0.0044650417413765325\n",
      "\n",
      "time of error check of native model:  0.28348565101623535 seconds\n",
      "time of error check of onnx model:  7.10001802444458 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the above command should have generated the ONNX export as well as a configuration file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Mar 19 15:19 .\n",
      "drwxr-xr-x 3 root root 4096 Mar 19 15:18 ..\n",
      "drwxr-xr-x 2 root root 4096 Mar 19 15:19 1\n",
      "-rw-r--r-- 1 root root  570 Mar 19 15:19 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-onnx-trt-fp16/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.1 : Enable TensorRT Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to enable TensorRT, we need to add an additional section to the \"config.pbtxt\" configuration file. In particular, we need to add an additional segment to the <code>optimization</code> section:\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-trt-fp16/config.pbtxt) to enable TensorRT. Feel free to look at the [solution](solutions/ex-1-4-1_config.pbtxt) as needed.\n",
    "2. Once you have saved your changes (Main menu: File -> Save File), move the folder to the model repository using the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mv ./candidatemodels/bertQA-onnx-trt-fp16 model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Execute our profiling tool in the next cell and investigate the impact on performance. This could take a while to start, as we are waiting for the server to migrate the model to TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 207.956 infer/sec. p85 latency: 38152 usec\n",
      "  Pass [2] throughput: 211.93 infer/sec. p85 latency: 38392 usec\n",
      "  Pass [3] throughput: 207.92 infer/sec. p85 latency: 38346 usec\n",
      "  Client: \n",
      "    Request count: 157\n",
      "    Throughput: 209.269 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 26.33 infer/sec\n",
      "    p50 latency: 37743 usec\n",
      "    p85 latency: 38383 usec\n",
      "    p90 latency: 38626 usec\n",
      "    p95 latency: 39015 usec\n",
      "    p99 latency: 41040 usec\n",
      "    Avg gRPC time: 38015 usec (marshal 5 usec + response wait 38009 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1256\n",
      "    Execution count: 157\n",
      "    Successful request count: 157\n",
      "    Avg request latency: 37681 usec (overhead 47 usec + queue 20 usec + compute input 118 usec + compute infer 37488 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 211.921 infer/sec. p85 latency: 76263 usec\n",
      "  Pass [2] throughput: 211.918 infer/sec. p85 latency: 76404 usec\n",
      "  Pass [3] throughput: 211.92 infer/sec. p85 latency: 76716 usec\n",
      "  Client: \n",
      "    Request count: 159\n",
      "    Throughput: 211.919 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 26.66 infer/sec\n",
      "    p50 latency: 75806 usec\n",
      "    p85 latency: 76597 usec\n",
      "    p90 latency: 76714 usec\n",
      "    p95 latency: 76951 usec\n",
      "    p99 latency: 77130 usec\n",
      "    Avg gRPC time: 75434 usec (marshal 5 usec + response wait 75428 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1272\n",
      "    Execution count: 159\n",
      "    Successful request count: 159\n",
      "    Avg request latency: 75016 usec (overhead 45 usec + queue 37082 usec + compute input 21 usec + compute infer 37860 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 207.947 infer/sec. p85 latency: 114616 usec\n",
      "  Pass [2] throughput: 207.943 infer/sec. p85 latency: 115440 usec\n",
      "  Pass [3] throughput: 211.916 infer/sec. p85 latency: 115370 usec\n",
      "  Client: \n",
      "    Request count: 157\n",
      "    Throughput: 209.269 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 26.33 infer/sec\n",
      "    p50 latency: 114326 usec\n",
      "    p85 latency: 115342 usec\n",
      "    p90 latency: 115467 usec\n",
      "    p95 latency: 115776 usec\n",
      "    p99 latency: 116018 usec\n",
      "    Avg gRPC time: 113973 usec (marshal 5 usec + response wait 113967 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1256\n",
      "    Execution count: 157\n",
      "    Successful request count: 157\n",
      "    Avg request latency: 113409 usec (overhead 46 usec + queue 75257 usec + compute input 20 usec + compute infer 38077 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 207.937 infer/sec. p85 latency: 153883 usec\n",
      "  Pass [2] throughput: 207.935 infer/sec. p85 latency: 153868 usec\n",
      "  Pass [3] throughput: 211.93 infer/sec. p85 latency: 153248 usec\n",
      "  Client: \n",
      "    Request count: 157\n",
      "    Throughput: 209.267 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 26.32 infer/sec\n",
      "    p50 latency: 152885 usec\n",
      "    p85 latency: 153803 usec\n",
      "    p90 latency: 153933 usec\n",
      "    p95 latency: 154506 usec\n",
      "    p99 latency: 155137 usec\n",
      "    Avg gRPC time: 152032 usec (marshal 5 usec + response wait 152026 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1256\n",
      "    Execution count: 157\n",
      "    Successful request count: 157\n",
      "    Avg request latency: 151552 usec (overhead 46 usec + queue 113353 usec + compute input 22 usec + compute infer 38123 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 207.953 infer/sec. p85 latency: 191181 usec\n",
      "  Pass [2] throughput: 211.885 infer/sec. p85 latency: 191428 usec\n",
      "  Pass [3] throughput: 207.919 infer/sec. p85 latency: 191628 usec\n",
      "  Client: \n",
      "    Request count: 157\n",
      "    Throughput: 209.252 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 26.32 infer/sec\n",
      "    p50 latency: 190440 usec\n",
      "    p85 latency: 191443 usec\n",
      "    p90 latency: 191623 usec\n",
      "    p95 latency: 191827 usec\n",
      "    p99 latency: 192289 usec\n",
      "    Avg gRPC time: 189408 usec (marshal 6 usec + response wait 189401 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1256\n",
      "    Execution count: 157\n",
      "    Successful request count: 157\n",
      "    Avg request latency: 188919 usec (overhead 48 usec + queue 150824 usec + compute input 22 usec + compute infer 38017 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 211.955 infer/sec. p85 latency: 229939 usec\n",
      "  Pass [2] throughput: 207.926 infer/sec. p85 latency: 230349 usec\n",
      "  Pass [3] throughput: 207.931 infer/sec. p85 latency: 230973 usec\n",
      "  Client: \n",
      "    Request count: 157\n",
      "    Throughput: 209.271 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 26.33 infer/sec\n",
      "    p50 latency: 229425 usec\n",
      "    p85 latency: 230399 usec\n",
      "    p90 latency: 230530 usec\n",
      "    p95 latency: 230973 usec\n",
      "    p99 latency: 231631 usec\n",
      "    Avg gRPC time: 228039 usec (marshal 6 usec + response wait 228032 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1256\n",
      "    Execution count: 157\n",
      "    Successful request count: 157\n",
      "    Avg request latency: 227527 usec (overhead 46 usec + queue 189281 usec + compute input 20 usec + compute infer 38172 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 211.947 infer/sec. p85 latency: 269678 usec\n",
      "  Pass [2] throughput: 207.935 infer/sec. p85 latency: 269750 usec\n",
      "  Pass [3] throughput: 207.934 infer/sec. p85 latency: 269524 usec\n",
      "  Client: \n",
      "    Request count: 157\n",
      "    Throughput: 209.272 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 26.33 infer/sec\n",
      "    p50 latency: 268566 usec\n",
      "    p85 latency: 269776 usec\n",
      "    p90 latency: 269921 usec\n",
      "    p95 latency: 270258 usec\n",
      "    p99 latency: 270554 usec\n",
      "    Avg gRPC time: 266930 usec (marshal 5 usec + response wait 266924 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1256\n",
      "    Execution count: 157\n",
      "    Successful request count: 157\n",
      "    Avg request latency: 266404 usec (overhead 46 usec + queue 228038 usec + compute input 21 usec + compute infer 38291 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 207.949 infer/sec. p85 latency: 308513 usec\n",
      "  Pass [2] throughput: 207.921 infer/sec. p85 latency: 308987 usec\n",
      "  Pass [3] throughput: 207.9 infer/sec. p85 latency: 309604 usec\n",
      "  Client: \n",
      "    Request count: 156\n",
      "    Throughput: 207.923 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 26.16 infer/sec\n",
      "    p50 latency: 307912 usec\n",
      "    p85 latency: 309208 usec\n",
      "    p90 latency: 309560 usec\n",
      "    p95 latency: 309756 usec\n",
      "    p99 latency: 310143 usec\n",
      "    Avg gRPC time: 306206 usec (marshal 5 usec + response wait 306200 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1248\n",
      "    Execution count: 156\n",
      "    Successful request count: 156\n",
      "    Avg request latency: 305692 usec (overhead 45 usec + queue 267198 usec + compute input 20 usec + compute infer 38421 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 207.952 infer/sec. p85 latency: 347414 usec\n",
      "  Pass [2] throughput: 207.919 infer/sec. p85 latency: 348511 usec\n",
      "  Pass [3] throughput: 203.946 infer/sec. p85 latency: 349029 usec\n",
      "  Client: \n",
      "    Request count: 155\n",
      "    Throughput: 206.606 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 25.99 infer/sec\n",
      "    p50 latency: 346888 usec\n",
      "    p85 latency: 348501 usec\n",
      "    p90 latency: 348857 usec\n",
      "    p95 latency: 349074 usec\n",
      "    p99 latency: 350219 usec\n",
      "    Avg gRPC time: 345076 usec (marshal 5 usec + response wait 345070 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1240\n",
      "    Execution count: 155\n",
      "    Successful request count: 155\n",
      "    Avg request latency: 344658 usec (overhead 45 usec + queue 306094 usec + compute input 21 usec + compute infer 38489 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 207.944 infer/sec. p85 latency: 387730 usec\n",
      "  Pass [2] throughput: 207.922 infer/sec. p85 latency: 388080 usec\n",
      "  Pass [3] throughput: 203.944 infer/sec. p85 latency: 388748 usec\n",
      "  Client: \n",
      "    Request count: 155\n",
      "    Throughput: 206.603 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 25.99 infer/sec\n",
      "    p50 latency: 386834 usec\n",
      "    p85 latency: 388273 usec\n",
      "    p90 latency: 388691 usec\n",
      "    p95 latency: 389228 usec\n",
      "    p99 latency: 389764 usec\n",
      "    Avg gRPC time: 384491 usec (marshal 6 usec + response wait 384484 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1240\n",
      "    Execution count: 155\n",
      "    Successful request count: 155\n",
      "    Avg request latency: 384083 usec (overhead 46 usec + queue 345387 usec + compute input 20 usec + compute infer 38622 usec + compute output 7 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 209.269 infer/sec, latency 38383 usec\n",
      "Concurrency: 2, throughput: 211.919 infer/sec, latency 76597 usec\n",
      "Concurrency: 3, throughput: 209.269 infer/sec, latency 115342 usec\n",
      "Concurrency: 4, throughput: 209.267 infer/sec, latency 153803 usec\n",
      "Concurrency: 5, throughput: 209.252 infer/sec, latency 191443 usec\n",
      "Concurrency: 6, throughput: 209.271 infer/sec, latency 230399 usec\n",
      "Concurrency: 7, throughput: 209.272 infer/sec, latency 269776 usec\n",
      "Concurrency: 8, throughput: 207.923 infer/sec, latency 309208 usec\n",
      "Concurrency: 9, throughput: 206.606 infer/sec, latency 348501 usec\n",
      "Concurrency: 10, throughput: 206.603 infer/sec, latency 388273 usec\n",
      "CPU times: user 449 ms, sys: 120 ms, total: 569 ms\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency = \"10\"\n",
    "batchSize = \"8\"\n",
    "print(\"Running: \" + modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open questions now\n",
    "* latency change, especially across larger concurrency runs? \n",
    "* level of bandwidth change observed? \n",
    "* Why did the ONNX model timeout at concurrency of less than 10? How does the TensorRT latency at concurrency 10 compare to latency of pure ONNX runtime at an earlier concurrency?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully deployed an NLP model to Triton Server with TorchScript and applied both reduced precision and TensorRT optimizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
