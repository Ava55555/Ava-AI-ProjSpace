Summary of Projects in this folder 
----------------------------------
ðŸš€ðŸŽ‰ Check it out ðŸ‘‡....

Advanced LLMs
---------------
Architecting an end-to-end LLM pipeline from scratch
Implement Semantic Caching with Redisand GCP

Optimizing and Deploying Large Language Models
Quantization methods (GPTQ, AWQ, GGUF) to reduce model size
Hosting options: VMs with RunPod, serverless platforms like AWS Bedrock and Azure OpenAI, local deployment
Setting up performant inference endpoints with Hugging Face Transformers

Advanced_Retrieval_Langsmith
----------------------------
LangSmith and Evaluation

Agentic_RAG
-----------
Agentic RAG Powered by LangChain

Fine Tuning Embedding Models
----------------------------
leverage quantization via the bitsandbytes library for loading models onto your local machine
Fine-tune embeddings for domain adaptation

Model Performance Optimization
------------------------------
Neural network optimization
Explore the model: Deployed an NLP model to Triton Server with TorchScript and applied both reduced precision and TensorRT optimizations.
Host the model
Server performance - configured optimizations and profiled the model
Using the model -Monitor real-time inference in action with a question-answering NLP task.

RAG evaluation
--------------
Evaluate RAG pipelines using a powerful open-source tool called "Ragas"

Transformer Based NLP
---------------------
How Language models with self supervision have moved beyond basic Transformer to BERT and evr larger models
Prodcution deployment considerations

Fine tuning Llama3
------------------
fine-tune a GPT-style model on the summarization task using QLoRA/LoRA
